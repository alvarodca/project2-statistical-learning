---
title: "Notebook"
output: html_document
date: "2024-11-25"
---

# 1.Set Up

```{r}
rm(list = ls())

# For reproducibility purposes
set.seed(123)

# List of required libraries
required_libraries <- c("skimr", "ggplot2","dplyr", "VIM", "dbscan", "RColorBrewer", "isotree" ,"e1071", "bestNormalize","caret", "GGally", "corrplot", "ellipse")

# Install and load libraries
for (lib in required_libraries) {
  if (!require(lib, character.only = TRUE)) {
    install.packages(lib, dependencies = TRUE)
    library(lib, character.only = TRUE)
  }
}
```

We will start by importing the dataset and interpreting both its structure and content.

```{r}
data <- read.csv("train.csv", sep = ",")
skim(data)
```

With a first visualization of the dataset we can see that 43 of the columns are categorical while 38 of them are numerical. We can also observe some missing values as well as some redundant columns, like Id.

### 1.1 Data Splitting

Before doing any other analysis or operations with the data we will split into training and testing sets. This is because we will treat the test data as "future" data, and looking at it during development could unintentionally bias the models.

Furthermore, since the dataset is relatively small (\~1500 observations), we will reserve a 20% of data for testing The remaining 80% will be used for training, since we will also use cross-validation to evaluate any model. This ensures that we have sufficient data to have robust predictivions and evaluations.

```{r}
split <- createDataPartition(data$SalePrice, p = 0.8, list = FALSE)
training <- data[split,]
testing <- data[-split,]
nrow(training)
nrow(testing)
```

# 2. Data Preprocessing

Before training any model, it is mandatory to perform a thorough preprocessing. This does not only consist on correcting any errors on the data, but also on transforming it into a format that the algorithms that will be applied can handle. Moreover, it is also a key step to improve model performance, since eliminating noise will positively impact the precision of predictions.

## 2.1 NA Values

We observed before that there were some columns that had many NA values. We will use a barplot to visualize this better and decide what to do with them.

```{r}
# Barplot with the number of NA values
barplot(colMeans(is.na(training)), las=2)
```

Based on the previous plot and the skim function used we can see that there are some variables which have NA values for most if not almost all of the observations.

```{r}
# Finding out the columns with a high number of NA values
colnames(training)[which(colMeans(is.na(training))>0.75)]
```

These columns are Alley, PoolQC, Fence and MiscFeature

Alley refers to the type of alley access to the property. It can be Gravel or Paved, NA means no alley, almost all houses have an NA value here, we don't think that the type of alley is of great importance, so we will remove it.

PoolQC refers to the quality of the swimming pool, as most houses don't have a pool and there are other variables such as PoolArea which refers to the dimension of the swimming pool, we will remove it.

Fence refers to the quality of the fencing, following a similar explanation as before, it has too many NA values, it won't provide much information

MiscFeature mentions other additional features houses may have or not, such as tennis courts. As this are really exclusive and as with the other variables there are a lot of missing values we will delete it.

```{r}
# Deleting the variables
na_columns <- c("Alley","PoolQC","Fence", "MiscFeature")
training <- training %>% select(-all_of(na_columns))

# Obtaining the remaining number of NA values
ncol(training %>% filter(if_any(everything(), is.na)))
```

```{r}
# Obtaining the remaining number of NA values
nrow(training %>% filter(if_any(everything(), is.na)))
```

We can see that we still have many observations with NA values, therefore we will now concentrate on replacing the missing values, taking into account their importance and if the missing variable is categorical or not.

To solve missing NA values we will separate the dataset into both numerical and categorical variables. Then we will work with both split datasets, and after dealing with the missing values we will merge them back.

```{r}
# Separating our dataset into both numerical and categorical data
numeric_data <- select_if(training, is.numeric)
numeric_data <- numeric_data %>% select(-Id)
categorical_data <- select_if(training, is.character)
```

We will start working with numerical data as it will be a little bit easier. Firstly we will have a look at the different columns

```{r}
skim(numeric_data)
```

We can see that only 3 variables contain missing values, these are LotFrontage, MasVnrArea, GarageYrBlt.

Most of the observations have a value for LotFrontage which is the variable with more NA values, this is a high completion rate and therefore we will not remove it.

Finally, to solve these missing values we will use imputation by the median. The reason behind this is that most of the data is complete and we don't think that doing any fancy imputation will drastically improve any future models in this case.

```{r}
# Replace NA values with the median of each column
numeric_data <- numeric_data %>%
  mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))

# Number of rows with NA values 
nrow(numeric_data %>% filter(if_any(everything(), is.na)))
```

Now that the NA values are solved for numeric data, we will have to check the categorical data.

```{r}
skim(categorical_data)
```

The variables with missing values are MasVnrType, all the variables referring to Bsmt which refers to the basement, one NA value in Electrical. Garage variables also have NA values. FirePlaceQu is the variable with most missing values.

Some of the NA values in the basement, garage and FirePlaceQu refer to that the house has none of these elements (as specified in the dataset webpage) , therefore we will change the NA values to None.

```{r}
# Choosing the columns to change from NA to None
na_columns <- c("BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1","BsmtFinType2", "FireplaceQu", "GarageType", "GarageFinish", "GarageQual","GarageCond")


categorical_data[na_columns][is.na(categorical_data[na_columns])] <- "None" 
```

As for MasVnrType there are only 8 missing values, Electrical has only one missing value. Due to this we will use KNN. This finds the most similar observations based on the other variables and imputes the missing value accordingly. With this small number of NAs we reduce the risk of introducing bias or overfitting in the imputation.

```{r}
# Replace NA values with the mode of each column
imputed_categorical_data <- VIM::kNN(categorical_data,
                                variable = c("MasVnrType","Electrical"),
                                k = 10)

# KNN creates a new dataset with extra columns which indicate the rows that have been changed, therefore we will have to change the imputed rows for our original rows
categorical_data$MasVnrType <- imputed_categorical_data$MasVnrType
categorical_data$Electrical <- imputed_categorical_data$Electrical

# Number of rows with NA values 
nrow(categorical_data %>% filter(if_any(everything(), is.na)))

```

Now that there are no NA values we can merge the training data back together.

```{r}
# Combining both datasets and obtaining a general view of it
training <- cbind(numeric_data, categorical_data)
skim(training)
```

## 2.2 Exploratory Data Analysis

We will deal with outliers later as they are a bit complicated to deal with in this dataset due to the distribution of the variables, this distribution can be seen in the graph we made below.

If we do the scaling to remove outliers we will not be able to correctly visualize the data. Moreover, visualizing our data could be crucial for potentially finding out some outliers.

We want to generate some insights to obtain information about the Sale Price and how it relates to the remaining variables.

```{r}
ggplot(training, aes(x = SalePrice, y = GrLivArea))+
  geom_point(aes(color = OverallQual))+
  scale_x_continuous(labels = scales::comma) +
  facet_wrap(~ HouseStyle)+
    labs(
      title="Relation of Area, Quality and House Style in relation to the Sale Price",
      x = "Sale Price",
      y = "Area",
      color = "Quality"
    )+
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )
  
```

This first graph compares the square feet area of the House in relation to the Sale Price. This is differentiated by color with respect to the quality of the house and all the houses are differentiated by the different dwelling styles which refer mostly to the different floors the house has.

This graph clearly shows a positive correlation between the Sale Price and the Area, which makes sense, the bigger the house the pricier the house. Both of these variables seem to also be correlated with the Overall Quality as usually more expensive houses or bigger houses have really good quality.

In relation to the dwelling, most of the houses are either 1 story or 2 story, we see that the price, area and quality is not really affected by the type of the dwelling, although it helps us to differentiate the data and we can see some points for example in the 2 story there is a house with really high area and moderate price which could be an outlier.

We will now compare the Sale Price respect to the year it was built and differentiating by the Type of Dwelling

```{r}

ggplot(training, aes(x = YearBuilt, y = SalePrice, color = BldgType)) +
  geom_smooth(method = "loess", se = FALSE) +
  labs(title = "Trend of SalePrice Over YearBuilt by BldgType", 
       x = "Year Built", 
       y = "Sale Price") +
  theme_minimal()

```

We can see that throughout the years the single family house (1Fam) was the most common house sold. Up until the 1980s two family homes (2fmCon) were also sold and sometimes reached higher sale prices than the single family, but from then on it seems they stopped being built (or we not have this info in our dataset). For the houses built from the 1980s onwards there was an increase in the different types and its price at which the houses were sold.

With respect to the price we can see a high increase in all the different housing categories when the 2000s are reached which could be due to inflation, an overall economic growth and a growth of urban population.

Moreover, there are also sudden decreases in price for example for Townhouse End Units (TwnhsE) which could be due to a lower demand or different and more affordable housing options.

To conclude our EDA we will do a correlation plot for the numeric data.

```{r}
cor_matrix = cor(numeric_data)

corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 90)

```

We observe that some columns like OverallQual (overall quality of the house) have a really strong positive correlation with the predictor, SalePrice. Moreover, many of them have little to no correlation with the predictor, so we will have to deal with them later since they may be noise affecting future models. Finally there are very few negative correlations between columns which is interesting.

## 2.3 Outliers

We will now deal with potential outliers, since as we discussed before, handling them properly can improve our predictions. Firstly, we will visualize the distribution of each of the variables separating both numerical and categorical data again.

```{r}
for (col_name in names(numeric_data)) {
  hist(
    numeric_data[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}
  
```

As we can see, there is a lot of variability in the data, some of them are left-skewed while some right-skewed. Also there are some discrete variables like FullBath that represents the number of full baths in the house. Before we can handle the outliers we will have to scale and transform the data.

Because of the wide variety of distributions in the data we cannot make every transformation by hand. Instead we will use a library that, based on mathematical values, applies the best transformation possible to the column and then scales it, so that it is as close to a Gaussian distribution as possible. The library had some problems with the transformation of some columns that had many 0 values, so we separated them and scaled them separately.

```{r}
# Problematic columns (because of errors with bestNormalize)
problematic_cols <- c("LowQualFinSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch")

# Separating the problematic columns
problematic_data <- numeric_data[, problematic_cols]
new_numeric_data <- numeric_data[, !(names(numeric_data) %in% problematic_cols)]

# Scaling the remaining numeric data
scaled_num_data <- as.data.frame(apply(new_numeric_data, 2, function(col) {
  bestNormalize(col, allow_orderNorm = TRUE)$x.t
}))
scaled_num_data <- as.data.frame(scale(scaled_num_data)) 

# Scaling the problematic columns separately
scaled_problematic <- as.data.frame(scale(problematic_data)) 

# Combining the scaled dataframes
final_scaled_data <- cbind(scaled_num_data, scaled_problematic)

# Plotting again the distributions
for (col_name in names(final_scaled_data)) {
  hist(
    final_scaled_data[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}
```

Now we can see that most of the data resembles a $N(\mu=0, \sigma=1)$ so the scaling has been done correctly and we can fully focus on outliers.

For the detection of outliers we will use three algorithms following the logic of an ensemble, in order to have a more robust prediction. We will only use numerical columns, since clustering algorithms do not handle categorical variables properly without transformations.

The first algorithm we will use is DBSCAN. It is a clustering algorithm that identifies clusters based on the density of data points. Points are classified into three categories: core points, border points and noise, based on this:

$$N_\epsilon(p) = \{q \in D \mid d(p,q) \leq \epsilon \}$$

Where $N_\epsilon$ is the $\epsilon$-neighborhood for a point. With this, a point $p$ is a core point if:

$$|N_\epsilon(p)| \geq minPts$$

Where $minPts$ is a hyper-parameter, the number of points for a region to be considered dense. Clusters are then formed by expanding the $\epsilon$-neighborhood of core points iteratively. Those observation not in this neighborhood will be the oultiers.

With this explanation we see the importance of picking the right $\epsilon$ value (eps) and the number of neighbors (minPts). Because of this we will calculate the k-distances plot and use the elbow method to determine the eps. As minPts we will use 4, since it is a reasonable number in our case.

```{r}
# Calculate k-distances (e.g., 4th nearest neighbor)
kNNdistplot(final_scaled_data, k = 10)
abline(h = 6.5, col = "red", lty = 2)  # Choose the threshold for eps
```

We see that there is an elbow at 6.5, so this is the value we will use for eps. With this we can finally train a DBSCAN model and plot our predictions for outliers.

```{r}
# Training the model
model <- dbscan(final_scaled_data, eps = 6.5, minPts = 10)

# Performing pca for plotting
pca <- prcomp(final_scaled_data, center = TRUE)
# Get the first two principal components
pca_data <- as.data.frame(pca$x)  # The principal components scores
# Add cluster labels to the PCA data
pca_data$dbscan <- model$cluster

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(dbscan))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "DBSCAN Clustering with Outliers",
       subtitle = "Red points indicate outliers (cluster 0)",
       color = "Cluster") +
  theme_minimal()

# Calculate the total number of outliers detected
sum(pca_data$dbscan == 0)
```

The second method for outlier detection we will use is Isolation Forests. They are a tree-based unsupervised learning method specific for anomally detection. They use the idea that outliers are usually more 'isolated' than normal observation. Each tree in the forest splits the data by selecting a random feature and a split value.

The path length $h(x)$ of a point $x$ is the number of splits required to isolate the observation. Since outliers are far from dense clusters, usually they have shorter path lengths. With this we calculate the anomaly score:

$$
s(x) = 2^{-\frac{H(x)}{c(n)}}
$$

Where $c(n)$ is the average path length for a normal data point in a dataset of $n$ observations, and $H(x)$ is the average path legth of the point. The closest this anomaly score is to 1 the more likely it is an anomaly. Therefore we will look at the anomaly score to see where the outliers lie.

```{r}
# Setting ndim=1 to only take into account one feature at each split 
# for simplicity, and ntrees as 100 to make an ensemble for more robust
# predictions
isolation_model <- isolation.forest(final_scaled_data, ndim = 1, ntrees = 100)
outlier_score <- predict(isolation_model, final_scaled_data, type = "score")

# Visualizing the different values
hist(outlier_score, breaks = 20, main = "Distribution of Anomaly Scores")
```

We can see that most of the observations have an anomaly score between 0 and 0.5, so according to this model a value of around 0.5 should be the boundary between outliers and the rest of points. We can see that there is one observation far away from the rest while there are some observations between 0.52 and 0.6 which are few but could also be outliers.

We will set the inlier proportion to 0.95, meaning that we will assume that 5% of our data are outliers. With this and the previous model we trained, we can predict outliers.

```{r}
# Stablishing a threshold for the highest values
threshold <- quantile(outlier_score, 0.95)
outliers <- which(outlier_score >= threshold)
print(unique(outliers))

pca_data$iso <- ifelse(1:nrow(pca_data) %in% outliers, 0, 1)
```

The third model we will use for outlier detection is OCSVM (One-Class Support Vector Machine). It is an unsupervised learning algorithm that tries to separate the anomalies from the rest of the data with a hyperplane (a line in 2D) that maximizes the distance between the outliers and the other points.

Mathematically, OCSVM maps points $x_i$ into higher-dimensional feature space using a kernel function $\phi(x)$. This is done to capture non-linear relationships (similar to what we did in Kernel K-Means). It then finds a decision function:

$$
f(x) = \langle w,  \phi(x) \rangle - \rho
$$

Where $w$ is the weight vector of the hyperplane, $\rho$ is the offset and $\langle w, \phi(x) \rangle$ is the inner product in the feature space. Then the optimization proble minimizes the modulus of $w$ while allowing a small fraction $\upsilon$ (this controls the proportion of outliers) of points to lie outside the boudary. If $f(x_i) < 0$ then $x_i$ is classified as an outlier.

```{r}
# Fit the One-Class SVM
ocsvm_model <- svm(
  final_scaled_data,
  type = "one-classification",
  kernel = "radial",  # RBF kernel
  gamma = 1 / ncol(data),  # Default gamma (1 / num_features)
  nu = 0.05  # Proportion of outliers you expect
)

# Get predictions
predictions <- predict(ocsvm_model, final_scaled_data)

# Add the numeric predictions as a column to the PCA data
pca_data$ocsvm <- ifelse(predictions, 1, 0)

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(ocsvm))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "OCSVM Outliers Detection",
       subtitle = "Red points indicate outliers",
       color = "Outlier") +
  theme_minimal()

sum(pca_data$ocsvm == F)
```

We keep using the dataset based on the PCA we have done (for plotting mainly), this dataset contains three extra columns which correspond to each of the outlier detection models we have made. In each of these columns, an observation can take a value of either 0 or 1, if it has a 0, it is considered an outlier.

Therefore, we created another column that decided if an observatoin had at least two 0's, in other words, at least two models consider the observation to be an outlier, this point will be eliminated from our dataset. As different models assure that this points are atypical points we can discard them from our dataset confidently.

```{r}

# Creating the final column based on the values
pca_data$outliers <- ifelse(rowSums(pca_data[, c("iso", "ocsvm", "dbscan")]) >= 2, 1, 0)

sum(pca_data$outliers == 0)

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(outliers), shape=as.factor(iso))) +
  geom_point(size = 3, alpha = 0.65) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "Ensemble Outliers Detection",
       subtitle = "Red points indicate outliers",
       color = "Outlier",
       shape = "Isolation Forest Prediction") +
  theme_minimal()
```

The previous graph compares the ensemble's prediction with the isolation forest, where the distinction by color is the outlier detection done by the ensemble and the distinction by shape is done by the Isolation Forest.

We can see that most of the outliers are predicted the same way, although there are some discrepancies. These are mainly between the Isolation Forest and the other models, since their approach is quite different.

With this, we will now proceed to eliminate the outliers.

```{r}
# Obtaining the indices of the outliers
outlier_indices <- which(pca_data$outliers == 0)
# This will return 51 indices 

# Removing this indices from the dataset
final_scaled_data <- final_scaled_data[-outlier_indices,]
categorical_data <- categorical_data[-outlier_indices,]
```

For our categorical data we have around 39 variables and our numerical data has 37 variables. This yields up to 76 columns. This is a really high dimension and this could really affect our future analysis. Because of this, we will reduce our dataset to obtain the most important variables which affect our target variable sale price.

## 2.4 Feature Extraction

First of all we will have to handle categorical data,

We will obtain the most important features with the help of RFE method.

RFE or Recursive Feature Elimination builds many models based on the target variable. It will remove variables based on their importance calculated with the Gini index. With this reduced dataset it will create another model and again remove the least important variable. This process is repeated until the best value is found.

After applying this process we will obtain an optimal subset of predictors which contribute the most to the target variable.

When using this method there are two hyperparameters to take into account, the type of model that will be created, which in our case due to that we have categorical data we will clearly use Random Forests. Apart from that, we can also select the number features that will be used in each of the models.

As we want to reduce our dimensionality based on the output of the RFE we will select an specific amount of variables which return a good value and do not make us loose a lot of information about the target variable .

```{r}
# We will have to separate the predictors and target variable
num_predictors <- final_scaled_data[,-which(names(final_scaled_data) == "SalePrice")]
target_variable <- final_scaled_data$SalePrice

control <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
# rfFuncs refers to that random forests will be done
# cv refers to that cross validation will be done
# Number referes to the number of folds created by cross validation

# Numerical columns
rfe_num <- rfe(num_predictors, target_variable, sizes = seq(5, 35, by = 5), rfeControl = control)
# Performs RFE, the variable size takes into account the number of features we will keep. This process tries models with all different sizes, computes the optimal size based on the accuracy and returns the best size.
print(rfe_num)


# Categorical columns 
rfe_cat <- rfe(categorical_data, target_variable, sizes = seq(5, 15, by = 2), rfeControl = control)

print(rfe_cat)
```

The first column variables, refers to the number of variables each of the models that have been created has. RMSE is the root mean squared error which is the average of the error between predicted and actual values. The lower this value is, the better. The R squared shows the proportion of the variance of the target variable, the closer to one, the better. For MAE (Mean Absolute Error), we want to obtain a low number. The RMSESD, which is the standard deviation of the RMSE, lower values indicate a grater performance. RsquaredSD, the standard deviation of R squared. Same as the previous variables, we want a low value. MAESD, the standard deviation of the MAE, same as before, the lower the better. Finally, the selected column shows an asterisk in the final chosen model.

In this case we will focus on the first three columns we want to minimize the number of variables which return a low prediction error and explain a high percentage of the variance of the sale price.

We can see that for numerical data the model for 15 variables is not the best result-wise but it returns a really good value despite it being a really big dimensionality reduction, therefore we will choose 15 variables.

Moreover, for categorical data, the lowest MSE and MAE is obtained by the model with 9 variables, it is also a decent value for the R squared based on the fact that it is only 9 variables.

```{r}
full_scaled_data = cbind(final_scaled_data, categorical_data)

# Selecting 15 features for numerical data
selected_num_features <- predictors(rfe_num)[1:15]
print(selected_num_features)

# Selecting 9 features for categorical data
selected_cat_features <- predictors(rfe_cat)[1:9]
print(selected_cat_features)

# Combining both results
selected_features <- c(selected_num_features, selected_cat_features)

# Obtaining our reduced dataset with the most important variables.
reduced_data <- full_scaled_data[,selected_features]
reduced_data$SalePrice <- target_variable

```

We reduced our numerical features to 15 variables and our categorical data to 9 variables. Going from 76 variables to 24, where we added the target variable.

This resulted in a final dataset of 1118 observations and 25 variables which is a really big change with respect to what we previously had, and will improve our future analysis.

## 2.5 Encoding

After having reduced the dimensionality of categorical features, we can proceed with encoding.

Most statistical and machine learning models require the predictors to be in some sort of numeric format. In this case, most of the algorithms and models that we will use later in our project need data to be numerical. There are many encoding methods that we could use to convert categorical variables into numerical ones, so we have to choose wisely.

```{r}
data_enc <- reduced_data
data_not_enc <- reduced_data

# Convert all character columns to factors
data_not_enc[] <- lapply(data_not_enc, function(col) {
  if (is.character(col)) as.factor(col) else col
})

skim(reduced_data)
# Get unique values for each categorical column
lapply(reduced_data[, sapply(reduced_data, is.character)], unique)

```

Firstly we can see that there are many variables that have a predetermined order, like ExterQual (Excellent, Good...). As there is a clear distinction between the values of the variables we will use label encoding, that assigns to each instance a number so that there is a scale.

```{r}
data_enc$ExterQual <- as.integer(factor(data_enc$ExterQual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))
data_enc$FireplaceQu <- as.integer(factor(data_enc$FireplaceQu, levels = c("None", "Po", "Fa", "TA", "Gd", "Ex")))
data_enc$BsmtQual <- as.integer(factor(data_enc$BsmtQual, levels = c("None", "Fa", "TA", "Gd", "Ex")))
data_enc$KitchenQual <- as.integer(factor(data_enc$KitchenQual, levels = c("Fa", "TA", "Gd", "Ex")))
```

There are 5 variables left that have not any order in their categories, for them we have two options. Firstly, we could use one-hot encoding (creating columns with 0s and 1s depending on the unique values of each column), but this is not feasable for most variables, since they have many unique values, and would increase the number of columns highly.

Instead, we will use target encoding, where each unique value is replaced with the mean SalePrice of it, since we know from the feature extraction that these variables are important. This aproach has a risk, which is data leakage, since these columns may "give too much information" about the target value to a future model. For this we will use cross validation, in order not to have this data leakage.

First we will create a function that will carry out this target encoding.

```{r}
# Function for target encoding with cross-validation
target_encode_cv <- function(data, target_col, cat_columns, n_folds = 5) {
  folds <- createFolds(data[[target_col]], k = n_folds)  # Create folds
  
  # Initializing an empty dataframe to store encoded results
  data_encoded <- data
  
  # Loop through each categorical column
  for (col in cat_columns) {
    # Initialize the encoded column
    data_encoded[[paste0(col, '_enc')]] <- NA
    
    # Process each fold
    for (fold in 1:n_folds) {
      # Split into training and validation folds
      val_indices <- folds[[fold]]
      train_fold <- data[-val_indices, ]
      val_fold <- data[val_indices, ]
      
      # Compute means in the training fold for the target variable
      means <- train_fold %>%
        group_by(.data[[col]]) %>%
        summarize(mean_target = mean(.data[[target_col]], na.rm = TRUE))
      
      # Merge the means with the validation fold
      val_fold <- val_fold %>%
        left_join(means, by = col) %>%
        mutate(mean_target = ifelse(is.na(mean_target), mean(train_fold[[target_col]], na.rm = TRUE), mean_target))
      
      # Assigning the encoded values
      data_encoded[val_indices, paste0(col, '_enc')] <- val_fold$mean_target
    }
  }
  
  return(data_encoded)
}

```

We now have the code that encodes our desired columns based on target encoding, therefore we will now apply it to our dataset and finish with the preprocessing.

```{r}
# Remaining variables
categorical_columns <- c("HouseStyle", "MSZoning", "Neighborhood", "BldgType", "GarageType")
# Applying the function
df_encoded <- target_encode_cv(data_enc, target_col = "SalePrice", cat_columns = categorical_columns)

# Switching the variables
data_enc$HouseStyle <- df_encoded$HouseStyle_enc
data_enc$Neighborhood <- df_encoded$Neighborhood_enc
data_enc$MSZoning <- df_encoded$MSZoning_enc
data_enc$BldgType <- df_encoded$BldgType_enc
data_enc$GarageType <- df_encoded$GarageType_enc

# Scaling the data becuase of the encodings
data_enc[, (ncol(data_enc) - 10):(ncol(data_enc) - 1)] <- scale(data_enc[, (ncol(data_enc) - 10):(ncol(data_enc) - 1)])

# Obtaining a general view of our final encoded dataset
skim(data_enc)
```

After all of the preprocessing, it is interesting to do again another correlation plot to see how our data has changed after the preprocessing and to perhaps identify some mistakes we did.

```{r}
cor_matrix = cor(data_enc)

corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 90)

```

We now can see that most of the variables have a strong positive relation with the predictor, with this we are sure that the feature extraction we did was successful. Furthermore, after the encoding we see that many variables that were categorical have a strong correlation with the SalePrice, such as Neighborhood. This tells us that doing the encoding to handle categorical variables was worth it.

Many of the algorithms we will use are specific for classification, while our predictor is a numerical variable. To deal with this problem we will divide the price of each house into 5 categories (Very Low, Low, Medium, High, Very High). We will make this classification based on quantiles so that we have balanced categories, which will make it easier to train posterior models.

```{r}
# Quantile-based bins (e.g., quartiles or deciles)
num_categories <- 5  # Use 5 for quintiles, 4 for quartiles, 10 for deciles
data_enc$SalePrice_Category <- cut(data_enc$SalePrice, 
                                       breaks = quantile(data_enc$SalePrice, probs = seq(0, 1, length.out = num_categories + 1)),
                                       labels = paste0("Category_", 1:num_categories),
                                       include.lowest = TRUE)

# Assign descriptive labels
labels <- c("Very Low", "Low", "Medium", "High", "Very High")
data_enc$SalePrice_Category <- cut(data_enc$SalePrice, 
                                breaks = quantile(data_enc$SalePrice, probs = seq(0, 1, length.out = 6)),
                                labels = labels,
                                include.lowest = TRUE)

# For not encoded data
# Quantile-based bins (e.g., quartiles or deciles)
num_categories <- 5  # Use 5 for quintiles, 4 for quartiles, 10 for deciles
data_not_enc$SalePrice_Category <- cut(data_not_enc$SalePrice, 
                                       breaks = quantile(data_not_enc$SalePrice, probs = seq(0, 1, length.out = num_categories + 1)),
                                       labels = paste0("Category_", 1:num_categories),
                                       include.lowest = TRUE)

# Assign descriptive labels
labels <- c("Very Low", "Low", "Medium", "High", "Very High")
data_not_enc$SalePrice_Category <- cut(data_not_enc$SalePrice, 
                                breaks = quantile(data_not_enc$SalePrice, probs = seq(0, 1, length.out = 6)),
                                labels = labels,
                                include.lowest = TRUE)

# Plot categories
ggplot(data_enc, aes(x = SalePrice_Category, fill = SalePrice_Category)) + 
  geom_bar()
```

# 3. Classification

We will look at each algorithm starting with the simpler ones and building up to the more complex and precise ones. Along with each prediction, we will follow up with the interpretability of the results while trying to minimize the prediction error.

## 3.1 Decision Trees

### 3.1.1 Introduction

## 3.2 K-Nearest Neighbors

## 3.3 Linear Discriminant Analysis

## 3.4 Quadratic Discriminant Analysis

## 3.5 Naive Bayes

## 3.6 Logistic Regression

## 3.7 Support Vector Machines

## 3.8 Random Forests

## 3.9 Gradient Boosting 
