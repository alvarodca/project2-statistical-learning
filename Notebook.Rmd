---
title: "Notebook"
output: html_document
date: "2024-11-25"
---

# 1.Set Up

```{r}
rm(list = ls())

# For reproducibility purposes
set.seed(123)

# List of required libraries
required_libraries <- c("skimr", "ggplot2","dplyr", "VIM", "dbscan", "RColorBrewer", "isotree" ,"e1071", "bestNormalize","caret")

# Install and load libraries
for (lib in required_libraries) {
  if (!require(lib, character.only = TRUE)) {
    install.packages(lib, dependencies = TRUE)
    library(lib, character.only = TRUE)
  }
}
```

We will start obtaining the datasets and interpreting what we have as well as making some decision which could be useful for our future analysis.

```{r}
data <- read.csv("train.csv", sep = ",")
```

```{r}
skim(training)
```

With a first visualization of the dataset we can see that 43 of the columns are categorical while 38 of them are numerical.

### Data Splitting

As we have around 1400 observations we will divide data into a 50/50 split for testing and training.

```{r}
split <- createDataPartition(data$SalePrice, p = 0.5, list = FALSE)
training <- data[split,]
testing <- data[-split,]
nrow(training)
nrow(testing)
```

# 2. Data Preprocessing

We can see that there are some columns that have many NA values, we will use a barplot to visualize this better and decide whether we should remove columns or not.

## 2.1 NA Values

```{r}
# Plotting a barplot with the number of NA values
barplot(colMeans(is.na(training)), las=2)
```

Based on the previous plot and the skim function used we can see that there are some variables which have NA values for most if not almost all of the observations.

```{r}
# Finidng out the columns with a high number of NA values
colnames(training)[which(colMeans(is.na(training))>0.75)]
```

These columns are Alley, PoolQC, Fence and MiscFeature

Alley refers to the type of alley access to the property. It can be Gravel or Paved, NA means no alley, almost all houses have an NA value here, we don't think that the type of alley is of great importance, so we will remove it.

PoolQC refers to the quality of the swimming pool, as most houses don't have a pool and there are other variables such as PoolArea which refers to the dimension of the swimming pool, we will remove it.

Fence refers to the quality of the fencing, following a similar explanation as before, it has too many NA values, it won't provide much information

MiscFeature mentions other additional features houses may have or not, such as tennis courts. As this are really exclusive and as with the other variables there are a lot of missing values we will delete it.

```{r}
# Deleting the variables
na_columns <- c("Alley","PoolQC","Fence", "MiscFeature")
training <- training %>% select(-all_of(na_columns))

# Obtaining the remaining number of NA values
ncol(training %>% filter(if_any(everything(), is.na)))
```

```{r}
# Obtaining the remaining number of NA values
nrow(training %>% filter(if_any(everything(), is.na)))
```

We can see that we still have many observations with NA values, therefore we will now concentrate on replacing the missing values, taking into account their importance and if the missing variable is categorical or not.

To solve missing NA values we will separate the dataset into both numerical and categorical variables, work with both split datasets and then after dealing with the missing values we will merge them.

```{r}
# Separating our dataset into both numerical and categorical data
numeric_data <- select_if(training, is.numeric)
numeric_data <- numeric_data %>% select(-Id)
categorical_data <- select_if(training, is.character)
```

We will start working with numerical data as it will be a little bit easier, firstly we will have a look at the different columns

```{r}
skim(numeric_data)
```

We can see that only 3 variables contain missing values, these are LotFrontage ,MasVnrArea ,GarageYrBlt

Most of the observations have a value for LotFrontage which is the variable with more NA values, this is a high completion rate and therefore we will not remove it.

To solve these missing values we will use imputation by the median .

```{r}
# Replace NA values with the median of each column
numeric_data <- numeric_data %>%
  mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))

# Number of rows with NA values 
nrow(numeric_data %>% filter(if_any(everything(), is.na)))
```

Now that the NA values are solved for numeric data, we will have to check for categorical data.

```{r}
skim(categorical_data)
```

The variables with missing values are MasVnrType, all the variables referring to Bsmt which refers to the basement, one NA value in Electrical. Garage variables also have NA values. FirePlaceQu is the variable with most missing values.

As for MasVnrType there are only 8 missing values , Electrical has only one missing value. Due to this we will use KNN to find the most similar points to our missing value and then imputing its most similar value.

Some of the NA values in the basement, garage and FirePlaceQu refer to that the house has none of these elements, therefore we will change the NA values from NA to None. We might use this variables later with encoding to obtain a better analysis and prediction.

```{r}
# Choosing the columns to change from NA to None
na_columns <- c("BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1","BsmtFinType2", "FireplaceQu", "GarageType", "GarageFinish", "GarageQual","GarageCond")


categorical_data[na_columns][is.na(categorical_data[na_columns])] <- "None" 
```

Applying KNN to do the imputation for the remaining NA values.

```{r}
# Replace NA values with the mode of each column

imputed_categorical_data <- VIM::kNN(categorical_data,
                                variable = c("MasVnrType","Electrical"),
                                k = 10)

# KNN creates a new dataset with extra columns which indicate the rows that have been changed, therefore we will have to change the imputed rows for our original rows

categorical_data$MasVnrType <- imputed_categorical_data$MasVnrType
categorical_data$Electrical <- imputed_categorical_data$Electrical

# Number of rows with NA values 
nrow(categorical_data %>% filter(if_any(everything(), is.na)))

```

```{r}
# Combining both datasets and obtaining a general view of it
data <- cbind(numeric_data, categorical_data)
skim(data)
```

## 2.2 Ouliers

We will now deal with potential outliers, to do this we will first visualize the distribution each of the variables follow separating both numerical and categorical data

```{r}
for (col_name in names(numeric_data)) {
  hist(
    numeric_data[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}
  
```

```{r}


# Step 1: Define the columns with issues
problematic_cols <- c("LowQualFinSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch")

# Step 2: Separate the problematic columns from the numeric data
problematic_data <- numeric_data[, problematic_cols]
new_numeric_data <- numeric_data[, !(names(numeric_data) %in% problematic_cols)]

# Step 3: Scale the remaining numeric data
scaled_num_data <- as.data.frame(apply(new_numeric_data, 2, function(col) {
  bestNormalize(col, allow_orderNorm = TRUE)$x.t
}))
scaled_num_data <- as.data.frame(scale(scaled_num_data)) # Standardize

# Step 4: Scale the problematic columns separately
scaled_problematic <- as.data.frame(scale(problematic_data)) # Standardize without transformation

# Step 5: Combine the scaled dataframes
final_scaled_data <- cbind(scaled_num_data, scaled_problematic)

for (col_name in names(final_scaled_data)) {
  hist(
    final_scaled_data[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}
```

```{r}
# Calculate k-distances (e.g., 4th nearest neighbor)
kNNdistplot(final_scaled_data, k = 4)
abline(h = 6, col = "red", lty = 2)  # Choose the threshold for eps
```

```{r}


model <- dbscan(final_scaled_data, eps = 6, minPts = 4)

pca <- prcomp(final_scaled_data, center = TRUE)

# Get the first two principal components
pca_data <- as.data.frame(pca$x)  # The principal components scores

# Add cluster labels to the PCA data
pca_data$dbscan <- model$cluster

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(dbscan))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "DBSCAN Clustering with Outliers",
       subtitle = "Red points indicate outliers (cluster 0)",
       color = "Cluster") +
  theme_minimal()

sum(pca_data$dbscan == 0)

# Identify Outliers
# outliers <- data[model$cluster == 0, ]
# cleaned_data <- data[model$cluster != 0, ]
```

Using an Isolation Forest model for outlier detection

```{r}
isolation_model <- isolation.forest(final_scaled_data, ndim = 1, ntrees = 100)
# We could change both ndim and ntrees for different results, but these results are good

outlier_score <- predict(isolation_model, final_scaled_data, type = "score")

# Visualiing the different values
hist(outlier_score, breaks = 20, main = "Distribution of Anomaly Scores")
```

We can see that most of the observations have a value between 0 and 0.5, according to this model a value of 0.5 is the boundary between outliers and the rest of points. Values closer to 1 are potential outliers. We can see that there is one observation far away from the rest while there are some observations between 0.52 and 0.6 which are few but could also be outliers.

```{r}
# Stablishing a threshold for the highest values
threshold <- quantile(outlier_score, 0.95)
outliers <- which(outlier_score >= threshold)
print(unique(outliers)) # DEVUELVE EL MISMO VALOR DOS VECES, POR ESO UNIQUE

pca_data$iso <- ifelse(1:nrow(pca_data) %in% outliers, 0, 1)
```

```{r}

# Fit the One-Class SVM
ocsvm_model <- svm(
  final_scaled_data,
  type = "one-classification",
  kernel = "radial",  # RBF kernel
  gamma = 1 / ncol(data),  # Default gamma (1 / num_features)
  nu = 0.05  # Proportion of outliers you expect
)

# Get predictions
predictions <- predict(ocsvm_model, final_scaled_data)

# Add the numeric predictions as a column to the PCA data
pca_data$ocsvm <- ifelse(predictions, 1, 0)

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(ocsvm))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "OCSVM Outliers Detection",
       subtitle = "Red points indicate outliers",
       color = "Outlier") +
  theme_minimal()

sum(pca_data$ocsvm == F)


```

```{r}
# Assuming your data has three columns: 'iso', 'ocsvm', and 'dbscan'
# Create the final column
pca_data$outliers <- ifelse(rowSums(pca_data[, c("iso", "ocsvm", "dbscan")]) >= 2, 1, 0)

sum(pca_data$outliers == 0)

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(outliers), shape=as.factor(iso))) +
  geom_point(size = 3, alpha = 0.65) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "OCSVM Outliers Detection",
       subtitle = "Red points indicate outliers",
       color = "Outlier") +
  theme_minimal()
```

This returns a variable which contains 0 is at least 2 of the 3 models consider the point to be an outlier, if not, it will return a 1. Therefore, we will proceed to obtain the indices of the rows with a value of 0 and remove them from both our categorical and numerical data.

```{r}
# Obtaining the indices of the outliers
outlier_indices <- which(pca_data$outliers == 0)
# This returns 63 indices 


# Removing this indices from the dataset
final_scaled_data <- final_scaled_data[-outlier_indices,]
categorical_data <- categorical_data[-outlier_indices,]
```

## 2.3 Feature Extraction

We have many variables and this really high dimensionality could be really bad in our future analysis. Therefore, we will use RFE (recursive feature elimination)

First of all we will have to handle categorical data,

```{r}
# We will first combine our data to do the feature selection.
full_data <- cbind(categorical_data,final_scaled_data)
```

RFE

RFE or Recursive Feature Elimination builds many models, in this case Random Forest models as it will handle categorical data. It will remove the least important features each time. It measures the quality of the variables based on Gini importance. The least imporant variables are eliminated and the process is repeated with the reduced dataset. This process is repeated until the best value is found.

```{r}
# We will have to separate the predictors and target variable
num_predictors <- final_scaled_data[,-which(names(final_scaled_data) == "SalePrice")]
target_variable <- final_scaled_data$SalePrice

control <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
# Functions indicates that random forest will be done here, cv refers to cross validation, which in this case as number is 5, it will separate the data in 5 splits.

# Numerical columns

rfe_num <- rfe(num_predictors, target_variable, sizes = seq(5, 35, by = 5), rfeControl = control)
# Performs RFE, the variable size takes into account the number of features we will keep. This process tries models with all different sizes, computes the optimal size based on the accuracy and returns the best size.
print(rfe_num)


# Categorical columns 
rfe_cat <- rfe(categorical_data, target_variable, sizes = seq(5, 15, by = 2), rfeControl = control)
# Performs RFE, the variable size takes into account the number of features we will keep. This process tries models with all different sizes, computes the optimal size based on the accuracy and returns the best size.
print(rfe_cat)

```

The first column variables, refers to the number of variables each of the models done has. RMSE is the root mean squared error which is the average of the error between predicted and actual values. The lower this value is, the better. The R squared shows the proportion of the variance of the target variable, the closer to one, the better. MAE (Mean Absolute Error), we want to obtain low number. The RMSESD, which is the standard deviation of the RMSE, lower values indicate a grater performance. RsquaredSD, the standard deviation of R squared. Same as the previous varaibles, we want a low value. MAESD, the standard deviation of the MAE, same as before, the lower the better. Finally, the selected column shows an asterisk in the final chosen model.

```{r}
selected_num_features <- predictors(rfe_num)[1:15]
print(selected_num_features)

selected_cat_features <- predictors(rfe_cat)[1:9]
print(selected_cat_features)

selected_features <- c(selected_num_features, selected_cat_features)

# Obtaining our reduced dataset with the most important variables.
reduced_data <- full_data[,selected_features]
reduced_data$SalePrice <- target_variable

```

## 2.4 Encoding

After having reduced the dimensionality of categorical features, we can proceed with encoding, in other terms, converting categorical variables into numerical ones. This is done because many algorithms that we are going to use later need all data to be numerical, so it is a mandatory step.

```{r}
skim(reduced_data)
# Get unique values for each categorical column
lapply(reduced_data[, sapply(reduced_data, is.character)], unique)

```

Firstly we can see that there are many variables that have a predetermined order, like ExterQual (Excellent, Good...). For this types of variables we will use label encoding, that assigns to each instance a number so that there is a scale.

```{r}
reduced_data$ExterQual <- as.integer(factor(reduced_data$ExterQual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))
reduced_data$FireplaceQu <- as.integer(factor(reduced_data$FireplaceQu, levels = c("None", "Po", "Fa", "TA", "Gd", "Ex")))
reduced_data$BsmtQual <- as.integer(factor(reduced_data$BsmtQual, levels = c("None", "Fa", "TA", "Gd", "Ex")))
reduced_data$KitchenQual <- as.integer(factor(reduced_data$KitchenQual, levels = c("Fa", "TA", "Gd", "Ex")))
reduced_data$GarageType <- as.integer(factor(reduced_data$GarageType, levels = c("None", "Detchd", "CarPort", "BuiltIn", "Basment", "Attchd", "2Types")))

```

There are 4 variables that have not any order in their categories, for them we have two options. Firstly, we could use one-hot encoding (creating columns with 0s and 1s depending on the unique values of each column), but this is not feasable for most variables, since they have unique values, and would increase the number of columns highly.

Instead, we will use target encoding, where each unique value is replaced with the mean SalePrice of it, since we know from the feature extraction that these variables are important. This aproach has a risk, and is data leakage, since these columns may "give too much information" about the target value to a future model. For this we will use cross validation, in order not to have this data leakage.

First we will create a function that will create this target encoding.

```{r}
# Function for target encoding with cross-validation
target_encode_cv <- function(data, target_col, cat_columns, n_folds = 5) {
  folds <- createFolds(data[[target_col]], k = n_folds)  # Create folds
  
  # Initialize an empty dataframe to store encoded results
  data_encoded <- data
  
  # Loop through each categorical column
  for (col in cat_columns) {
    # Initialize the encoded column
    data_encoded[[paste0(col, '_enc')]] <- NA
    
    # Process each fold
    for (fold in 1:n_folds) {
      # Split into training and validation folds
      val_indices <- folds[[fold]]
      train_fold <- data[-val_indices, ]
      val_fold <- data[val_indices, ]
      
      # Compute means in the training fold for the target variable
      means <- train_fold %>%
        group_by(.data[[col]]) %>%
        summarize(mean_target = mean(.data[[target_col]], na.rm = TRUE))
      
      # Merge the means with the validation fold
      val_fold <- val_fold %>%
        left_join(means, by = col) %>%
        mutate(mean_target = ifelse(is.na(mean_target), mean(train_fold[[target_col]], na.rm = TRUE), mean_target))
      
      # Assigning the encoded values
      data_encoded[val_indices, paste0(col, '_enc')] <- val_fold$mean_target
    }
  }
  
  return(data_encoded)
}

```

Applying the function to the variables, and changing the encoded rows to our original dataset

```{r}
# Remaining variables
categorical_columns <- c("HouseStyle", "MSZoning", "Neighborhood", "BldgType")
df_encoded <- target_encode_cv(reduced_data, target_col = "SalePrice", cat_columns = categorical_columns)

# Switching the variables
reduced_data$HouseStyle <- df_encoded$HouseStyle_enc
reduced_data$Neighborhood <- df_encoded$Neighborhood_enc
reduced_data$MSZoning <- df_encoded$MSZoning_enc
reduced_data$BldgType <- df_encoded$BldgType_enc

```

## 
