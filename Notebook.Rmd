---
title: "Notebook"
output: html_document
date: "2024-11-25"
---

# 1.Set Up

```{r}
rm(list = ls())

# For reproducibility purposes
set.seed(123)

# List of required libraries
required_libraries <- c("skimr", "ggplot2","dplyr", "VIM", "dbscan", "RColorBrewer", "isotree" ,"e1071", "bestNormalize","caret", "GGally", "corrplot", "rpart", "rpart.plot", "MASS","biotools", "klaR", "iml", "fastshap","randomForest", "gbm", "xgboost")

# Install and load libraries
suppressWarnings({ 

for (lib in required_libraries) {
  if (!require(lib, character.only = TRUE)) {
    install.packages(lib, dependencies = TRUE)
    library(lib, character.only = TRUE)
  }
}
})
```

We will start by importing the dataset and interpreting both its structure and content.

```{r}
data <- read.csv("train.csv", sep = ",")
skim(data)
```

With a first visualization of the dataset we can see that 43 of the columns are categorical while 38 of them are numerical. We can also observe some missing values as well as some redundant columns, like Id.

### 1.1 Data Splitting

Before doing any other analysis or operations with the data we will split into training and testing sets. This is because we will treat the test data as "future" data, and looking at it during development could unintentionally bias the models.

Furthermore, since the dataset is relatively small (\~1500 observations), we will reserve a 20% of data for testing The remaining 80% will be used for training, since we will also use cross-validation to evaluate any model. This ensures that we have sufficient data to have robust predictivions and evaluations.

```{r}
split <- createDataPartition(data$SalePrice, p = 0.8, list = FALSE)
training <- data[split,]
testing <- data[-split,]
nrow(training)
nrow(testing)
```

# 2. Data Preprocessing

Before training any model, it is mandatory to perform a thorough preprocessing. This does not only consist on correcting any errors on the data, but also on transforming it into a format that the algorithms that will be applied can handle. Moreover, it is also a key step to improve model performance, since eliminating noise will positively impact the precision of predictions.

## 2.1 NA Values

We observed before that there were some columns that had many NA values. We will use a barplot to visualize this better and decide what to do with them.

```{r}
# Barplot with the number of NA values
barplot(colMeans(is.na(training)), las=2)
```

Based on the previous plot and the skim function used we can see that there are some variables which have NA values for most if not almost all of the observations.

```{r}
# Finding out the columns with a high number of NA values
colnames(training)[which(colMeans(is.na(training))>0.75)]
```

These columns are Alley, PoolQC, Fence and MiscFeature

Alley refers to the type of alley access to the property. It can be Gravel or Paved, NA means no alley, almost all houses have an NA value here, we don't think that the type of alley is of great importance, so we will remove it.

PoolQC refers to the quality of the swimming pool, as most houses don't have a pool and there are other variables such as PoolArea which refers to the dimension of the swimming pool, we will remove it.

Fence refers to the quality of the fencing, following a similar explanation as before, it has too many NA values, it won't provide much information

MiscFeature mentions other additional features houses may have or not, such as tennis courts. As this are really exclusive and as with the other variables there are a lot of missing values we will delete it.

```{r}
# Deleting the variables
na_columns <- c("Alley","PoolQC","Fence", "MiscFeature")
training <- training[, !(colnames(training) %in% na_columns)]

# Obtaining the remaining number of NA values
ncol(training %>% filter(if_any(everything(), is.na)))
```

```{r}
# Obtaining the remaining number of NA values
nrow(training %>% filter(if_any(everything(), is.na)))
```

We can see that we still have many observations with NA values, therefore we will now concentrate on replacing the missing values, taking into account their importance and if the missing variable is categorical or not.

To solve missing NA values we will separate the dataset into both numerical and categorical variables. Then we will work with both split datasets, and after dealing with the missing values we will merge them back.

```{r}
# Separating our dataset into both numerical and categorical data
numeric_data <- select_if(training, is.numeric)
numeric_data <- numeric_data[,-1]
categorical_data <- select_if(training, is.character)
```

We will start working with numerical data as it will be a little bit easier. Firstly we will have a look at the different columns

```{r}
skim(numeric_data)
```

We can see that only 3 variables contain missing values, these are LotFrontage, MasVnrArea, GarageYrBlt.

Most of the observations have a value for LotFrontage which is the variable with more NA values, this is a high completion rate and therefore we will not remove it.

Finally, to solve these missing values we will use imputation by the median. The reason behind this is that most of the data is complete and we don't think that doing any fancy imputation will drastically improve any future models in this case.

```{r}
# Replace NA values with the median of each column
numeric_data <- numeric_data %>%
  mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))

# Number of rows with NA values 
nrow(numeric_data %>% filter(if_any(everything(), is.na)))
```

Now that the NA values are solved for numeric data, we will have to check the categorical data.

```{r}
skim(categorical_data)
```

The variables with missing values are MasVnrType, all the variables referring to Bsmt which refers to the basement, one NA value in Electrical. Garage variables also have NA values. FirePlaceQu is the variable with most missing values.

Some of the NA values in the basement, garage and FirePlaceQu refer to that the house has none of these elements (as specified in the dataset webpage) , therefore we will change the NA values to None.

```{r}
# Choosing the columns to change from NA to None
na_columns <- c("BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1","BsmtFinType2", "FireplaceQu", "GarageType", "GarageFinish", "GarageQual","GarageCond")


categorical_data[na_columns][is.na(categorical_data[na_columns])] <- "None" 
```

As for MasVnrType there are only 8 missing values, Electrical has only one missing value. Due to this we will use KNN. This finds the most similar observations based on the other variables and imputes the missing value accordingly. With this small number of NAs we reduce the risk of introducing bias or overfitting in the imputation.

```{r}
# Replace NA values with the mode of each column
imputed_categorical_data <- VIM::kNN(categorical_data,
                                variable = c("MasVnrType","Electrical"),
                                k = 10)

# KNN creates a new dataset with extra columns which indicate the rows that have been changed, therefore we will have to change the imputed rows for our original rows
categorical_data$MasVnrType <- imputed_categorical_data$MasVnrType
categorical_data$Electrical <- imputed_categorical_data$Electrical

# Number of rows with NA values 
nrow(categorical_data %>% filter(if_any(everything(), is.na)))

```

Now that there are no NA values we can merge the training data back together.

```{r}
# Combining both datasets and obtaining a general view of it
training <- cbind(numeric_data, categorical_data)
skim(training)
```

## 2.2 Exploratory Data Analysis

We will deal with outliers later as they are a bit complicated to deal with in this dataset due to the distribution of the variables, this distribution can be seen in the graph we made below.

If we do the scaling to remove outliers we will not be able to correctly visualize the data. Moreover, visualizing our data could be crucial for potentially finding out some outliers.

We want to generate some insights to obtain information about the Sale Price and how it relates to the remaining variables.

```{r}
ggplot(training, aes(x = SalePrice, y = GrLivArea))+
  geom_point(aes(color = OverallQual))+
  scale_x_continuous(labels = scales::comma) +
  facet_wrap(~ HouseStyle)+
    labs(
      title="Relation of Area, Quality and House Style in relation to the Sale Price",
      x = "Sale Price",
      y = "Area",
      color = "Quality"
    )+
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )
  
```

This first graph compares the square feet area of the House in relation to the Sale Price. This is differentiated by color with respect to the quality of the house and all the houses are differentiated by the different dwelling styles which refer mostly to the different floors the house has.

This graph clearly shows a positive correlation between the Sale Price and the Area, which makes sense, the bigger the house the pricier the house. Both of these variables seem to also be correlated with the Overall Quality as usually more expensive houses or bigger houses have really good quality.

In relation to the dwelling, most of the houses are either 1 story or 2 story, we see that the price, area and quality is not really affected by the type of the dwelling, although it helps us to differentiate the data and we can see some points for example in the 2 story there is a house with really high area and moderate price which could be an outlier.

We will now compare the Sale Price respect to the year it was built and differentiating by the Type of Dwelling

```{r}

ggplot(training, aes(x = YearBuilt, y = SalePrice, color = BldgType)) +
  geom_smooth(method = "loess", se = FALSE) +
  labs(title = "Trend of SalePrice Over YearBuilt by BldgType", 
       x = "Year Built", 
       y = "Sale Price") +
  theme_minimal()

```

We can see that throughout the years the single family house (1Fam) was the most common house sold. Up until the 1980s two family homes (2fmCon) were also sold and sometimes reached higher sale prices than the single family, but from then on it seems they stopped being built (or we not have this info in our dataset). For the houses built from the 1980s onwards there was an increase in the different types and its price at which the houses were sold.

With respect to the price we can see a high increase in all the different housing categories when the 2000s are reached which could be due to inflation, an overall economic growth and a growth of urban population.

Moreover, there are also sudden decreases in price for example for Townhouse End Units (TwnhsE) which could be due to a lower demand or different and more affordable housing options.

To conclude our EDA we will do a correlation plot for the numeric data.

```{r}
cor_matrix = cor(numeric_data)

corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 90)

```

We observe that some columns like OverallQual (overall quality of the house) have a really strong positive correlation with the predictor, SalePrice. Moreover, many of them have little to no correlation with the predictor, so we will have to deal with them later since they may be noise affecting future models. Finally there are very few negative correlations between columns which is interesting.

## 2.3 Outliers

We will now deal with potential outliers, since as we discussed before, handling them properly can improve our predictions. Firstly, we will visualize the distribution of each of the variables separating both numerical and categorical data again.

```{r}
for (col_name in names(numeric_data)) {
  hist(
    numeric_data[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}
  
```

As we can see, there is a lot of variability in the data, some of them are left-skewed while some right-skewed. Also there are some discrete variables like FullBath that represents the number of full baths in the house. Before we can handle the outliers we will have to scale and transform the data.

Because of the wide variety of distributions in the data we cannot make every transformation by hand. Instead we will use a library that, based on mathematical values, applies the best transformation possible to the column and then scales it, so that it is as close to a Gaussian distribution as possible. The library had some problems with the transformation of some columns that had many 0 values, so we separated them and scaled them separately.

```{r}
# Problematic columns (because of errors with bestNormalize)
problematic_cols <- c("LowQualFinSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch")

# Separating the problematic columns
problematic_data <- numeric_data[, problematic_cols]
new_numeric_data <- numeric_data[, !(names(numeric_data) %in% problematic_cols)]

# Scaling the remaining numeric data
scaled_num_data <- as.data.frame(apply(new_numeric_data, 2, function(col) {
  bestNormalize(col, allow_orderNorm = TRUE)$x.t
}))
scaled_num_data <- as.data.frame(scale(scaled_num_data)) 

# Scaling the problematic columns separately
scaled_problematic <- as.data.frame(scale(problematic_data)) 

# Combining the scaled dataframes
final_scaled_data <- cbind(scaled_num_data, scaled_problematic)

# Plotting again the distributions
for (col_name in names(final_scaled_data)) {
  hist(
    final_scaled_data[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}
```

Now we can see that most of the data resembles a $N(\mu=0, \sigma=1)$ so the scaling has been done correctly and we can fully focus on outliers.

For the detection of outliers we will use three algorithms following the logic of an ensemble, in order to have a more robust prediction. We will only use numerical columns, since clustering algorithms do not handle categorical variables properly without transformations.

The first algorithm we will use is DBSCAN. It is a clustering algorithm that identifies clusters based on the density of data points. Points are classified into three categories: core points, border points and noise, based on this:

$$N_\epsilon(p) = \{q \in D \mid d(p,q) \leq \epsilon \}$$

Where $N_\epsilon$ is the $\epsilon$-neighborhood for a point. With this, a point $p$ is a core point if:

$$|N_\epsilon(p)| \geq minPts$$

Where $minPts$ is a hyper-parameter, the number of points for a region to be considered dense. Clusters are then formed by expanding the $\epsilon$-neighborhood of core points iteratively. Those observation not in this neighborhood will be the oultiers.

With this explanation we see the importance of picking the right $\epsilon$ value (eps) and the number of neighbors (minPts). Because of this we will calculate the k-distances plot and use the elbow method to determine the eps. As minPts we will use 4, since it is a reasonable number in our case.

```{r}
# Calculate k-distances (e.g., 4th nearest neighbor)
kNNdistplot(final_scaled_data, k = 10)
abline(h = 6.5, col = "red", lty = 2)  # Choose the threshold for eps
```

We see that there is an elbow at 6.5, so this is the value we will use for eps. With this we can finally train a DBSCAN model and plot our predictions for outliers.

```{r}
# Training the model
model <- dbscan(final_scaled_data, eps = 6.5, minPts = 10)

# Performing pca for plotting
pca <- prcomp(final_scaled_data, center = TRUE)
# Get the first two principal components
pca_data <- as.data.frame(pca$x)  # The principal components scores
# Add cluster labels to the PCA data
pca_data$dbscan <- model$cluster

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(dbscan))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "DBSCAN Clustering with Outliers",
       subtitle = "Red points indicate outliers (cluster 0)",
       color = "Cluster") +
  theme_minimal()

# Calculate the total number of outliers detected
sum(pca_data$dbscan == 0)
```

The second method for outlier detection we will use is Isolation Forests. They are a tree-based unsupervised learning method specific for anomally detection. They use the idea that outliers are usually more 'isolated' than normal observation. Each tree in the forest splits the data by selecting a random feature and a split value.

The path length $h(x)$ of a point $x$ is the number of splits required to isolate the observation. Since outliers are far from dense clusters, usually they have shorter path lengths. With this we calculate the anomaly score:

$$
s(x) = 2^{-\frac{H(x)}{c(n)}}
$$

Where $c(n)$ is the average path length for a normal data point in a dataset of $n$ observations, and $H(x)$ is the average path legth of the point. The closest this anomaly score is to 1 the more likely it is an anomaly. Therefore we will look at the anomaly score to see where the outliers lie.

```{r}
# Setting ndim=1 to only take into account one feature at each split 
# for simplicity, and ntrees as 100 to make an ensemble for more robust
# predictions
isolation_model <- isolation.forest(final_scaled_data, ndim = 1, ntrees = 100)
outlier_score <- predict(isolation_model, final_scaled_data, type = "score")

# Visualizing the different values
hist(outlier_score, breaks = 20, main = "Distribution of Anomaly Scores")
```

We can see that most of the observations have an anomaly score between 0 and 0.5, so according to this model a value of around 0.5 should be the boundary between outliers and the rest of points. We can see that there is one observation far away from the rest while there are some observations between 0.52 and 0.6 which are few but could also be outliers.

We will set the inlier proportion to 0.95, meaning that we will assume that 5% of our data are outliers. With this and the previous model we trained, we can predict outliers.

```{r}
# Stablishing a threshold for the highest values
threshold <- quantile(outlier_score, 0.95)
outliers <- which(outlier_score >= threshold)
print(unique(outliers))

pca_data$iso <- ifelse(1:nrow(pca_data) %in% outliers, 0, 1)
```

The third model we will use for outlier detection is OCSVM (One-Class Support Vector Machine). It is an unsupervised learning algorithm that tries to separate the anomalies from the rest of the data with a hyperplane (a line in 2D) that maximizes the distance between the outliers and the other points.

Mathematically, OCSVM maps points $x_i$ into higher-dimensional feature space using a kernel function $\phi(x)$. This is done to capture non-linear relationships (similar to what we did in Kernel K-Means). It then finds a decision function:

$$
f(x) = \langle w,  \phi(x) \rangle - \rho
$$

Where $w$ is the weight vector of the hyperplane, $\rho$ is the offset and $\langle w, \phi(x) \rangle$ is the inner product in the feature space. Then the optimization proble minimizes the modulus of $w$ while allowing a small fraction $\upsilon$ (this controls the proportion of outliers) of points to lie outside the boudary. If $f(x_i) < 0$ then $x_i$ is classified as an outlier.

```{r}
# Fit the One-Class SVM
ocsvm_model <- svm(
  final_scaled_data,
  type = "one-classification",
  kernel = "radial",  # RBF kernel
  gamma = 1 / ncol(data),  # Default gamma (1 / num_features)
  nu = 0.05  # Proportion of outliers you expect
)

# Get predictions
predictions <- predict(ocsvm_model, final_scaled_data)

# Add the numeric predictions as a column to the PCA data
pca_data$ocsvm <- ifelse(predictions, 1, 0)

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(ocsvm))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "OCSVM Outliers Detection",
       subtitle = "Red points indicate outliers",
       color = "Outlier") +
  theme_minimal()

sum(pca_data$ocsvm == F)
```

We keep using the dataset based on the PCA we have done (for plotting mainly), this dataset contains three extra columns which correspond to each of the outlier detection models we have made. In each of these columns, an observation can take a value of either 0 or 1, if it has a 0, it is considered an outlier.

Therefore, we created another column that decided if an observatoin had at least two 0's, in other words, at least two models consider the observation to be an outlier, this point will be eliminated from our dataset. As different models assure that this points are atypical points we can discard them from our dataset confidently.

```{r}

# Creating the final column based on the values
pca_data$outliers <- ifelse(rowSums(pca_data[, c("iso", "ocsvm", "dbscan")]) >= 2, 1, 0)

sum(pca_data$outliers == 0)

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(outliers), shape=as.factor(iso))) +
  geom_point(size = 3, alpha = 0.65) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "Ensemble Outliers Detection",
       subtitle = "Red points indicate outliers",
       color = "Outlier",
       shape = "Isolation Forest Prediction") +
  theme_minimal()
```

The previous graph compares the ensemble's prediction with the isolation forest, where the distinction by color is the outlier detection done by the ensemble and the distinction by shape is done by the Isolation Forest.

We can see that most of the outliers are predicted the same way, although there are some discrepancies. These are mainly between the Isolation Forest and the other models, since their approach is quite different.

With this, we will now proceed to eliminate the outliers.

```{r}
# Obtaining the indices of the outliers
outlier_indices <- which(pca_data$outliers == 0)
# This will return 51 indices 

# Removing this indices from the dataset
final_scaled_data <- final_scaled_data[-outlier_indices,]
categorical_data <- categorical_data[-outlier_indices,]
```

For our categorical data we have around 39 variables and our numerical data has 37 variables. This yields up to 76 columns. This is a really high dimension and this could really affect our future analysis. Because of this, we will reduce our dataset to obtain the most important variables which affect our target variable sale price.

## 2.4 Feature Extraction

First of all we will have to handle categorical data,

We will obtain the most important features with the help of RFE method.

RFE or Recursive Feature Elimination builds many models based on the target variable. It will remove variables based on their importance calculated with the Gini index. With this reduced dataset it will create another model and again remove the least important variable. This process is repeated until the best value is found.

After applying this process we will obtain an optimal subset of predictors which contribute the most to the target variable.

When using this method there are two hyperparameters to take into account, the type of model that will be created, which in our case due to that we have categorical data we will clearly use Random Forests. Apart from that, we can also select the number features that will be used in each of the models.

As we want to reduce our dimensionality based on the output of the RFE we will select an specific amount of variables which return a good value and do not make us loose a lot of information about the target variable .

```{r}
# We will have to separate the predictors and target variable
num_predictors <- final_scaled_data[,-which(names(final_scaled_data) == "SalePrice")]
target_variable <- final_scaled_data$SalePrice

control <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
# rfFuncs refers to that random forests will be done
# cv refers to that cross validation will be done
# Number referes to the number of folds created by cross validation

# Numerical columns
rfe_num <- rfe(num_predictors, target_variable, sizes = seq(5, 35, by = 5), rfeControl = control)
# Performs RFE, the variable size takes into account the number of features we will keep. This process tries models with all different sizes, computes the optimal size based on the accuracy and returns the best size.
print(rfe_num)


# Categorical columns 
rfe_cat <- rfe(categorical_data, target_variable, sizes = seq(5, 15, by = 2), rfeControl = control)

print(rfe_cat)
```

The first column variables, refers to the number of variables each of the models that have been created has. RMSE is the root mean squared error which is the average of the error between predicted and actual values. The lower this value is, the better. The R squared shows the proportion of the variance of the target variable, the closer to one, the better. For MAE (Mean Absolute Error), we want to obtain a low number. The RMSESD, which is the standard deviation of the RMSE, lower values indicate a grater performance. RsquaredSD, the standard deviation of R squared. Same as the previous variables, we want a low value. MAESD, the standard deviation of the MAE, same as before, the lower the better. Finally, the selected column shows an asterisk in the final chosen model.

In this case we will focus on the first three columns we want to minimize the number of variables which return a low prediction error and explain a high percentage of the variance of the sale price.

We can see that for numerical data the model for 15 variables is not the best result-wise but it returns a really good value despite it being a really big dimensionality reduction, therefore we will choose 15 variables.

Moreover, for categorical data, the lowest MSE and MAE is obtained by the model with 9 variables, it is also a decent value for the R squared based on the fact that it is only 9 variables.

```{r}
full_scaled_data = cbind(final_scaled_data, categorical_data)

# Selecting 15 features for numerical data
selected_num_features <- predictors(rfe_num)[1:15]
print(selected_num_features)

# Selecting 9 features for categorical data
selected_cat_features <- predictors(rfe_cat)[1:9]
print(selected_cat_features)

# Combining both results
selected_features <- c(selected_num_features, selected_cat_features)

# Obtaining our reduced dataset with the most important variables.
reduced_data <- full_scaled_data[,selected_features]
reduced_data$SalePrice <- target_variable

```

We reduced our numerical features to 15 variables and our categorical data to 9 variables. Going from 76 variables to 24, where we added the target variable.

Making a feature importance plot based on the RFE results, it will rank in both datasets how early each variable was selected and afterwards create a plot

```{r eval=FALSE, include=FALSE}
#Obtaining the importance from both numerical and categorical data

num_importance <- varImp(rfe_num)$Importance

cat_importance <- varImp(rfe_cat)$Importance

feature_importance <- data.frame(

Feature = c(rownames(num_importance), rownames(cat_importance)),

Importance = c(num_importance$Overall, cat_importance$Overall

))

# Sorting the results to have them in order

feature_importance <- feature_importance[order(-feature_importance$Importance)]

ggplot(feature_importance, aes(x = reorder(Feature, Importance), y = Importance))+

geom_bar(stat = "identity", fill = "steelblue")


```

This resulted in a final dataset of 1118 observations and 25 variables which is a really big change with respect to what we previously had, and will improve our future analysis.

## 2.5 Encoding

After having reduced the dimensionality of categorical features, we can proceed with encoding.

Most statistical and machine learning models require the predictors to be in some sort of numeric format. In this case, most of the algorithms and models that we will use later in our project need data to be numerical. There are many encoding methods that we could use to convert categorical variables into numerical ones, so we have to choose wisely.

```{r}
data_enc <- reduced_data
data_not_enc <- reduced_data

# Convert all character columns to factors
data_not_enc[] <- lapply(data_not_enc, function(col) {
  if (is.character(col)) as.factor(col) else col
})

skim(reduced_data)
# Get unique values for each categorical column
lapply(reduced_data[, sapply(reduced_data, is.character)], unique)

```

Firstly we can see that there are many variables that have a predetermined order, like ExterQual (Excellent, Good...). As there is a clear distinction between the values of the variables we will use label encoding, that assigns to each instance a number so that there is a scale.

```{r}
data_enc$ExterQual <- as.integer(factor(data_enc$ExterQual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))
data_enc$FireplaceQu <- as.integer(factor(data_enc$FireplaceQu, levels = c("None", "Po", "Fa", "TA", "Gd", "Ex")))
data_enc$BsmtQual <- as.integer(factor(data_enc$BsmtQual, levels = c("None", "Fa", "TA", "Gd", "Ex")))
data_enc$KitchenQual <- as.integer(factor(data_enc$KitchenQual, levels = c("Fa", "TA", "Gd", "Ex")))
```

There are 5 variables left that have not any order in their categories, for them we have two options. Firstly, we could use one-hot encoding (creating columns with 0s and 1s depending on the unique values of each column), but this is not feasable for most variables, since they have many unique values, and would increase the number of columns highly.

Instead, we will use target encoding, where each unique value is replaced with the mean SalePrice of it, since we know from the feature extraction that these variables are important. This aproach has a risk, which is data leakage, since these columns may "give too much information" about the target value to a future model. For this we will use cross validation, in order not to have this data leakage.

First we will create a function that will carry out this target encoding.

```{r}
# Function for target encoding with cross-validation
target_encode_cv <- function(data, target_col, cat_columns, n_folds = 5) {
  folds <- createFolds(data[[target_col]], k = n_folds)  # Create folds
  
  # Initializing an empty dataframe to store encoded results
  data_encoded <- data
  
  # Loop through each categorical column
  for (col in cat_columns) {
    # Initialize the encoded column
    data_encoded[[paste0(col, '_enc')]] <- NA
    
    # Process each fold
    for (fold in 1:n_folds) {
      # Split into training and validation folds
      val_indices <- folds[[fold]]
      train_fold <- data[-val_indices, ]
      val_fold <- data[val_indices, ]
      
      # Compute means in the training fold for the target variable
      means <- train_fold %>%
        group_by(.data[[col]]) %>%
        summarize(mean_target = mean(.data[[target_col]], na.rm = TRUE))
      
      # Merge the means with the validation fold
      val_fold <- val_fold %>%
        left_join(means, by = col) %>%
        mutate(mean_target = ifelse(is.na(mean_target), mean(train_fold[[target_col]], na.rm = TRUE), mean_target))
      
      # Assigning the encoded values
      data_encoded[val_indices, paste0(col, '_enc')] <- val_fold$mean_target
    }
  }
  
  return(data_encoded)
}

```

We now have the code that encodes our desired columns based on target encoding, therefore we will now apply it to our dataset and finish with the preprocessing.

```{r}
# Remaining variables
categorical_columns <- c("HouseStyle", "MSZoning", "Neighborhood", "BldgType", "GarageType")
# Applying the function
df_encoded <- target_encode_cv(data_enc, target_col = "SalePrice", cat_columns = categorical_columns)

# Switching the variables
data_enc$HouseStyle <- df_encoded$HouseStyle_enc
data_enc$Neighborhood <- df_encoded$Neighborhood_enc
data_enc$MSZoning <- df_encoded$MSZoning_enc
data_enc$BldgType <- df_encoded$BldgType_enc
data_enc$GarageType <- df_encoded$GarageType_enc

# Scaling the data becuase of the encodings
data_enc[, (ncol(data_enc) - 10):(ncol(data_enc) - 1)] <- scale(data_enc[, (ncol(data_enc) - 10):(ncol(data_enc) - 1)])

# Obtaining a general view of our final encoded dataset
skim(data_enc)
```

After all of the preprocessing, it is interesting to do again another correlation plot to see how our data has changed after the preprocessing and to perhaps identify some mistakes we did.

```{r}
cor_matrix = cor(data_enc)

corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.srt = 90)

```

We now can see that most of the variables have a strong positive relation with the predictor, with this we are sure that the feature extraction we did was successful. Furthermore, after the encoding we see that many variables that were categorical have a strong correlation with the SalePrice, such as Neighborhood. This tells us that doing the encoding to handle categorical variables was worth it.

Many of the algorithms we will use are specific for classification, while our predictor is a numerical variable. To deal with this problem we will divide the price of each house into 5 categories (Very Low, Low, Medium, High, Very High). We will make this classification based on quantiles so that we have balanced categories, which will make it easier to train posterior models.

```{r}
# Quantile-based bins (e.g., quartiles or deciles)
num_categories <- 5  # Use 5 for quintiles, 4 for quartiles, 10 for deciles
data_enc$SalePrice_Category <- cut(data_enc$SalePrice, 
                                       breaks = quantile(data_enc$SalePrice, probs = seq(0, 1, length.out = num_categories + 1)),
                                       labels = paste0("Category_", 1:num_categories),
                                       include.lowest = TRUE)

# Assign descriptive labels
labels <- c("Very Low", "Low", "Medium", "High", "Very High")
data_enc$SalePrice_Category <- cut(data_enc$SalePrice, 
                                breaks = quantile(data_enc$SalePrice, probs = seq(0, 1, length.out = 6)),
                                labels = labels,
                                include.lowest = TRUE)

# For not encoded data
# Quantile-based bins (e.g., quartiles or deciles)
num_categories <- 5  # Use 5 for quintiles, 4 for quartiles, 10 for deciles
data_not_enc$SalePrice_Category <- cut(data_not_enc$SalePrice, 
                                       breaks = quantile(data_not_enc$SalePrice, probs = seq(0, 1, length.out = num_categories + 1)),
                                       labels = paste0("Category_", 1:num_categories),
                                       include.lowest = TRUE)

# Assign descriptive labels
labels <- c("Very Low", "Low", "Medium", "High", "Very High")
data_not_enc$SalePrice_Category <- cut(data_not_enc$SalePrice, 
                                breaks = quantile(data_not_enc$SalePrice, probs = seq(0, 1, length.out = 6)),
                                labels = labels,
                                include.lowest = TRUE)

# Plot categories
ggplot(data_enc, aes(x = SalePrice_Category, fill = SalePrice_Category)) + 
  geom_bar()
```

# 3. Classification

We will look at each algorithm starting with the simpler ones and building up to the more complex and precise ones. Along with each prediction, we will follow up with the interpretability of the results while trying to minimize the prediction error.

## 3.1 Decision Trees

### 3.1.1 Introduction

We will first look at decision trees, since they are a simple model. Although there are better algorithms for prediction, they are the foundations for more sophisticated methods. Furthermore, it is a white-box model, so it will help us understand how the predictions are done.

Mathematically, decision trees do a split in the data based on selected features. They do this split based on the Gini index, a metric that is similar to accuracy, with the splits that yield the best Gini index at the root of the tree.

### 3.1.2 Training

Since we are focusing on classification we will remove the salePrice from the dataset. Furthermore, since decision trees can handle categorical variables, and for understanding pourposes, we will use the dataset that is not encoded.

```{r}
salePrice <- data_enc$SalePrice
data_not_enc <- data_not_enc[,-(ncol(data_not_enc)-1)]
data_enc <- data_enc[,-(ncol(data_enc)-1)]
```

Even though we know that the decision tree is not going to be our best model, we will do a 10-fold cross validation to check more or less the performance of this simple algorithm. In addition, we will not do any hyper-parameter tuning since as we said our main focus with this algorithm is interpretability.

```{r}
# Setting up cross-validation
cv_folds <- trainControl(method = "cv", number = 10)
# Performing cross-validation
cv_model <- train(SalePrice_Category ~ ., data = data_not_enc, method = "rpart", trControl = cv_folds, tuneGrid = data.frame(cp = 0.01))
# Checking results
print(cv_model)

# Saving the final model for visualization
final_dt <- cv_model$finalModel
```

We see a 57% accuracy, which is not bad but there is room for improvement. Also we obtain a 0.47 Kappa value, which indicates a moderate agreement between the model's predictions and true labels.

### 3.1.3 Interpretation

Now, we can plot the decision tree so that we can understand how the predictions are done.

```{r}
png("decision_tree.png", width = 3000, height = 2000, res = 300)  
rpart.plot(final_dt, digits = 3)
dev.off()  
```

The first split done is by OverallQual, which measure the overall quality of the house, with this we can see that houses with a higher quality usually are pricier, as expected. In the right part of the tree, we observe another split by OverallQual, so it indicates that houses with a very high overall quality are the most expensive ones. Another split is by GrLivArea, which measures the above ground area of the house, meaning that bigger houses are more expensive. Finally, it splits by GarageType, with the houses that have a garage attached to home being more expensive.

In the other side of the decision tree, we observe a split with GarageArea, with observations with small garages being cheaper. We also see other splits, with BsmtFinSF1, with houses with smaller basements being less expensive. Finally we also see other splits, but all regarding the area of some part of the house, like the first floor.

With this we learn that the most important variables may be the overall quality of the house, as well as some other metrics related to the area of different pars of the house.

## 3.2 K-Nearest Neighbors

### 3.2.1 Introduction

The next algorithm we will use is k-Nearest Neighbors. It is a supervised learning model that performs classification. Mathematically, the algorithm calculates, for each new data point, its distance to each point in the training data. We then define a hyperparameter $k$, that selects the $k$ closest data points to this new observation. The most common class of these $k$ closest observations is then chosen as the predicted class.

Although this algorithm has some problems when dimensionality is high and it is a black-box model, we will use it to see how this types of algorithms perform in our data. This will help us understand whether the classes can be easily separable or not, as well as if more complex models are needed.

### 3.2.2 Training

Before training the final algorithm we have to choose the value for $k$, which is crucial for the algorithm to work optimally. For this we will perform hyper-parameter tuning for it, while measuring the efficiency of the model for each different value of $k$ with cross-validation.

```{r}
# Set up cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Define the tuning grid for k values
tune_grid <- expand.grid(k = 1:20)  # Try k values from 1 to 20 (you can adjust this range)

# Train the KNN model with cross-validation
knn_model <- train(SalePrice_Category ~ ., data = data_enc, method = "knn", trControl = train_control, tuneGrid = tune_grid)

# Display the results
print(knn_model)
```

### 3.2.3 Interpretation

We see that the optimal $k$, is 6 as it yields approximately a 66% accuracy and a kappa of 0.58. To visualize this model, we will train it on the whole dataset and perform PCA to visualize the classification in 2 dimensions.

```{r}
# Train the KNN model using caret
train_control <- trainControl(method = "none")  # No resampling
# Define the grid of values for k
tune_grid <- expand.grid(k = 6)  # Set k = 6
# Train the KNN model using caret on the entire dataset
final_knn <- train(SalePrice_Category ~ ., data = data_enc, method = "knn", trControl = train_control, tuneGrid = tune_grid)

# Perform PCA on the feature columns (exclude the target variable 'Class')
pca_result <- prcomp(data_enc[, -which(names(data_enc) == "SalePrice_Category")])
# Extract the first two principal components
pca_data <- data.frame(pca_result$x[, 1:2])  # First two PCs
pca_data$SalePrice_Category <- data_enc$SalePrice_Category  # Add class labels back to the data for visualization
# Get predicted class labels using the trained KNN model
pca_data$Predicted_Class_kNN <- predict(final_knn, newdata = data_enc)

# Visualize the PCA result with predicted class labels
ggplot(pca_data, aes(x = PC1, y = PC2, color = Predicted_Class_kNN, shape=SalePrice_Category)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "KNN Predictions in 2D (PCA)", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal() +
  scale_color_manual(values = rainbow(length(unique(pca_data$Predicted_Class_kNN))))  # Customize colors for predicted classes
```

We observe that most of the points in the class Very High have been classified correctly. We also see how Very Low has mostly been classified correctly, although there are more errors. On the other hand, the model struggles to distinguish between the 3 middle classes, which could be due to many factors, but one of them might be because the classification may be non-linear. To solve this we could use another type of distance in the k-NN such as Mahalanobis, or we could apply the kernel trick. Although these options might better the prediction slightly, we do not think that they would make the model perform much better, since the errors appear to be specific flaws of the k-NN algorithm itself. Instead, what we will do is focus more on the other models that might capture better this complexity of the data.

## 3.3 Quadratic Discriminant Analysis and Linear Discrimant Analysis

The next algorithms we will see are really similar, the biggest difference is that Linear Discriminant Analysis assumes that the different classes in the prediction all have the same covariance matrix. This is a really big difference, which could potentialy be one of the reasons why we chose one process over the other.

To see if the covariance matrices are the same or not we will use Box's M Test which tests if two or more covariance matrices are equal or not. This test assumes normality from the data if not it could have many errors.

The null hypothesis for this test states that the covariance matrices are equal, a really high p value indicates that this is true. We will use the library biotools that will perform this test. It compares the product of log determinants of the matrices to the log determinant of a pooled matrix (average of all the matrices), it then uses a chi squared approximation for the result. It is based on the following formula:

$$
M = \frac{n - k}{k \cdot (n - 1)} \sum_{i=1}^{k} (n_i - 1) \cdot \log \left( \det(\Sigma_i) \right) - \sum_{i=1}^{k} (n_i - 1) \cdot \log \left( \det(\Sigma) \right)
$$

Where $n$ is the number of observations, $k$ the number of classes, $\Sigma_i$ each covariance matrix, and finally the pooled matrix.

```{r}
boxM_test <- boxM(data_enc[,-which(names(data_enc) == "SalePrice_Category")],
                  grouping = data_enc$SalePrice_Category)
print(boxM_test)
```

This test clearly suggest that the covariance matrices are different therefore Quadratic Discrimant Analysis will more likely be better in our dataset. However, in this project we are covering many different supervised learning algorithms to study and understand them. This is why we will also cover Linear Discriminant Analysis, as Box M test may make some errors, and LDA could provide a similar classification while being less computationally expensive.

### 3.3.1 Quadratic Discriminant Analysis

#### 3.3.1.1 Introduction

The next algorithms we will see are really similar, the biggest difference is that Linear Discriminant Analysis assumes that the different classes in the prediction all have the same covariance matrix just as we said before.

The first algorithm we will use is Quadratic Discriminant Analysis (QDA) which is highly related to the next algorithm Linear Discrimant Analysis (LDA). It is a supervised learning technique used for classification. QDA assumes that all classes follow a normal distribution they have a mean vector and a covariance matrix (different for each class), it provides a division of the data with a quadratic decision boundary.

This algorithm has problems with really high dimensionality as it needs to estimate many vectors and covariance matrices. Moreover it is also really sensitive to deviations from Gaussian distribution. It is an unbiased estimator with a high variance. This problems will be tackled by LDA, which assumes the same covariances for all classes therefore we will also test it afterwards

Mathematically QDA is based on the Bayes Theorem which calculates the probability of an element belonging to an specific class, the mathematical formula estimates mean vectors, covariance matrices and prior probabilities.

The formula is given by

$$
\delta_k(x) = -\frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) + \log P(y = k)
$$

The first term corresponds to the logarithm of the determinant of the covariance matrix, the logarithm appears due to the derivation from Gaussian distribution. The second term measures how far is $x$ in relation to the mean multiplied by the covariance matrix, sometimes this matrix is not singular and therefore non invertible which is another problem. We want to minimize this first two terms as much as possible. Finally, the third term corresponds to the probability of an element y belonging to a class k.

This function is computed for every observation and each point is assigned to the highest value obtained by the function:

$$
\hat{y} = \arg \max_k \delta_k(x)\
$$

#### 3.3.1.2 Training

We will start creating the different models

```{r}
qda_model1 <- qda(SalePrice_Category ~ ., data_enc)
print(qda_model1)
```

We can see that the prior probabilities for each of the classes is around 0.2 for each of the classes, basically equal chances of being assigned to each of the classes. Moreover, we can see the mean value obtained for each of the variables with respect to the classes. Variables such as Area and Quality have a higher value for Very High sale price and small values for low prices which makes sense.

We will use cross validation and create a model based on the Quadratic Discriminant Analysis

```{r}
# Set up cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Train the QDA model with cross-validation
qda_model <- train(SalePrice_Category ~ ., data = data_enc, method = "qda", trControl = train_control)

# Display the results
print(qda_model)

```

We can see that the value obtained for accuracy is around 0.65 and around 0.55 for the kappa value which are good values and better than doing it by random chance but we could improve this values for sure.

We will now compare with LDA and then jump to conclusions

### 3.3.2 Linear Discriminant Analysis

#### 3.3.2.1 Introduction

We have already briefly introduced Linear Discriminant Analysis (LDA), it is also a supervised learning algorithm used for classification. It eliminates some of the QDA problems, it does this by introducing some bias to reduce variance thereby having a lower prediction error. The division of data is done linearly.

Same as QDA, it assumes a normal distribution but the main difference and the main reason why problems such as really high dimensionality are solved is that it assumes are equal. This is because if there are K classes we would only need K-1 discriminant functions as one class could be inferred from others.

$$
\delta'_k(x) = \delta_k(x) - \delta_K(x)
$$

Moreover it estimates p+1 parameters (transformation of the original parameters) to make the discriminant function. In Big O notation this is $O(p)$. But QDA has to estimate the mean for each parameter $p$, compute $\dfrac{p(p+1)}{2}$ covariance matrices and the probabilities. Which is why for a large $p$ LDA will be more practical. In Big O notation this is $O(p^2)$.

For classifying it follows the same procedure as QDA of the discriminant function. The biggest difference is just that it computes only one matrix. It will classify according to the highest value obtained.

#### 3.3.2.2 Training

```{r}
lda_model1 <- lda(SalePrice_Category~., data_enc)
print(lda_model1)
```

We can see in the output that the prior probabilities for each of the classes is around 0.2 for all of them. Apart from that we can also see the mean values for each class in the target variable. There are many negative values due to the normalization which has been done before. We can see that for example for the Very High Category, variables such as the area or the quality have high positive values whereas for the very low category this are negative.

We can also see the coefficients of linear discriminant which could be used to obtain the discriminant function or for dimensionality reduction as LDA is also used in this area. Based on this output we do not see much difference to the QDA done before, they provide really similar values

Now we will use cross validation to obtain the accuracy and kappa.

```{r}
# Set up cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Train the QDA model with cross-validation
lda_model <- train(SalePrice_Category ~ ., data = data_enc, method = "lda", trControl = train_control)

# Display the results
print(lda_model)
```

The accuracy is of around 0.69 and a value of 0.61 for kappa which is an increase with respect to the previous result we obtained

#### 3.3.2.3 Interpretation

The results obtained are surprising if we take Box's M test into consideration which returned a really low p value which indicated with great confidence that the covariance matrices were equal and this led us to believe that the results for QDA would be better.

After training both models we saw that this in fact was not true as LDA returns a better result. This could be due to potential overfitting from QDA as it estimates a separate covariance matrix and it might fit noise in the data.

Another possible option is that QDA is not regularized which is usually when the number of features is really high in relation to the number observations. Regular Discriminant Analysis offers a solution to this by reducing complexity and avoiding overfitting.

#### 3.3.2.4 Regularized Discriminant Analysis

Regularized Discriminant Analysis (RDA) is another version which could be used both for QDA and LDA. Its main idea is that it includes a regularization term to prevent overfitting by shrinking the covariance matrices, if elements from different classes overlap changing this term will improve accuracy, this parameter is gamma and it helps shrink towards a more stable value. If its value is 0, no regularization is done. Its formula is given by:

$$
\Sigma_k^\text{reg} = (1 - \gamma) \Sigma_k + \gamma \Sigma_{\text{global}}
$$

The first term is the regularization with gamma times the covariance matrix for each class $k$ and the regularization matrix times the pooled matrix (average of all the matrices). Apart from that, we can also regularize the mean vector with $\lambda$, it applies a penalty to mean vectors towards 0 or a common mean. A value of 0 indicates no regularization. Its formula is given by:

$$
\mu_k^\gamma = (1 - \gamma) \hat{\mu}_k + \gamma \bar{x}
$$

Regularization times the mean vector for each class added to lambda times the mean vector of all the classes. All this results in the final formula given by:

$$
\delta_k(x) = \ln \pi_k - \frac{1}{2} \ln |\Sigma_k^\lambda| - \frac{1}{2} (x - \mu_k^\gamma)^T \Sigma_k^{\lambda^{-1}} (x - \mu_k^\gamma)
$$

Each point will be assigned according to the discriminant function which returns a higher value.

In this code we could try many values for gamma and lambda, the complexity for this is $O(n^n)$as each value for lambda is tested with each value for gamma, therefore we chose a few numbers as it returns good values and does not take as much time.

```{r}
# Setting up cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Defining a tuning grid for both gamma and lambda
tune_grid <- expand.grid(gamma = seq(0, 1, by = 0.3), 
                         lambda = seq(0, 1, by = 0.3))  

# Training the regularized QDA model with cross-validation and tuning
rda_model <- train(SalePrice_Category ~ ., data = data_enc, method = "rda", 
                   trControl = train_control, tuneGrid = tune_grid)

# Displaying the results
print(rda_model$results)

# Obtaining the best parameters
best_result <- rda_model$results[which.max(rda_model$results$Accuracy), ]

# Returning the results
cat("Best Accuracy: ", best_result$Accuracy,"\n")
cat("Best Gamma: ", best_result$gamma,"\n")
cat("Best Lambda: ", best_result$lambda,"\n")
```

We can see that we obtained an accuracy of around 0.7 which is a clear improvement with respect to the previous QDA we performed and it obtains a greater accuracy than LDA. This supports the initial conclusion from the Box M test which stated different covariance matrices.

#### 3.3.2.5 Visualization of Results

We will know make some boxplots to create a visual representation of the results obtained.

```{r}
# Obtaining the performance metrics for all the different models
rda_res <- rda_model$resample
lda_res <- lda_model$resample
qda_res <- qda_model$resample

# Combining the results into one data frame
resample_data <- rbind(
  data.frame(Model = "RDA", Accuracy = rda_res$Accuracy),
  data.frame(Model = "LDA", Accuracy = lda_res$Accuracy),
  data.frame(Model = "QDA", Accuracy = qda_res$Accuracy)
)

# Plotting accuracy across folds
ggplot(resample_data, aes(x = Model, y = Accuracy, color = Model)) +
  geom_boxplot() +
  labs(title = "Cross-Validation Accuracy Comparison", x = "Model", y = "Accuracy") +
  theme_minimal()

```

The boxplots show that clearly the QDA has a wider spread and a lower median accuracy, however its value is really similar to the ones returned by the LDA. RDA seems to have bit better median and data the results are clearly as not as spread as the others. Its mean accuracy is fairly close to 0.7 which is not always the case for LDA and QDA which have mean values of around 0.67

```{r}
pca_data$Predicted_Class_rda <- predict(rda_model, newdata = data_enc)

# Visualize the PCA result with predicted class labels
ggplot(pca_data, aes(x = PC1, y = PC2, color = Predicted_Class_rda, shape=SalePrice_Category)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "RDA Predictions in 2D (PCA)", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal() +
  scale_color_manual(values = rainbow(length(unique(pca_data$Predicted_Class_rda))))  # Customize colors for predicted classes

```

## 3.4 Naive Bayes

### 3.4.1 Introduction

Naive Bayes is a conditional probability model based on the Bayes theorem. It assigns a probability that an observation $\mathbf{x}$ belongs to each possible class $C_k$. Mathematically:

$$
p(C_k\mid\mathbf{x})=\dfrac{p(C_k)p(\mathbf{x}\mid C_k)}{p(\mathbf{x})}
$$

Where $k$ is the number possible classes, and the vector $\mathbf{x}$ has $n$ features. In the end, the classifier decides the class based on the one that has the higher posterior probability. We then end up with the following prediction $\hat{y}$ :

$$
\hat{y} = \underset{k \in \{1, \dots, K\}}{\operatorname{argmax}} \ p(C_k) \prod_{i=1}^n p(x_i \mid C_k).
$$

Since we are mostly dealing with continuous data, and after the scaling and transformation, we will use Gaussian Naive Bayes. For this model we assume each class follows a normal distribution. Then, suppose observation value $v$, the probability density of it being in $C_k$ is computed as follows (based on the normal distribution):

$$
p(x = v \mid C_k) = \frac{1}{\sqrt{2\pi \sigma_k^2}} \, e^{-\frac{(v - \mu_k)^2}{2\sigma_k^2}}
$$

Where $\mu_k$ and $\sigma_k^2$ represent the mean and variance of the feature values in the class $C_k$.

Although it is a somewhat simple kind of model, it is good to try it before moving to more fancy methods. Furthermore, since the outputs are probabilities we can obtain valuable interpretations from it.

### 3.4.2 Training

We will firstly perform a small hyper-parameter tuning to optimize the model's results, while measuring how well it performs with a 10 fold cross-validation.

```{r}
# Define the trainControl object for cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Train Naive Bayes with caret
suppressWarnings({
nb_model <- train(SalePrice_Category ~ ., data = data_enc, method = "nb", trControl = train_control, tuneGrid = expand.grid(fL = c(0, 0.5, 1), usekernel = c(TRUE, FALSE), adjust = c(1, 1.5)))
})

# View the model
print(nb_model)
```

We observe that the best model has a 65% accuracy and a Kappa of 0.56 which has been very common in different algorithms throughout this project. This can be due to many reasons, but again there are inherent problems with the model and our data.

Firstly, each class is assumed to be normally distributed, and although we have scaled and transformed the data, we could observe how the middle class is normally distributed while maybe the "Very High" class is skewed. Moreover, we also assume independence between each class which is not correct, since for example the overall quality (OverallQual) and exterior quality (ExterQual) are clearly related, we observed this in the correlation plots we did right at the start.

### 3.4.3 Interpretation

Now we will move to the interpretation of the predictions. Here we print the mean and standard deviation for each class and feature. We would expect an increase or decrease in the mean as the price gets higher, while the standard deviation stays the same.

```{r}
# Train model with klaR to extract probabilities
nb_model_klar <- NaiveBayes(SalePrice_Category ~ ., data = data_enc)

# View conditional probabilities for features
nb_model_klar$tables
```

Although our initial guess might be correct at first we observe different cases where the means do not follow the suggested order, like in MSSubClass or OverallCond. Furthermore we see that there is a very noticeable gap in means between Very Low, Very High and the rest, while the middle 3 classes sometimes do not follow the order we discussed before, and they have really similar mean. This combined with the standard deviations, which are moreless the same for each class, makes the 3 middle classes very likely to be predicted wrong, just as we saw with each previous model.

We will finally do a plot with PCA to compare the predicted class with the real one. We will also highlight the points where the posterior probability for the predicted class has been less than 0.6, since they should be uncertain points for the model, which could be between classes.

```{r}
# Predict on the training dataset
suppressWarnings({
predictions <- predict(nb_model, data_enc, type = "prob")
})

# Identify uncertain points
uncertainty_threshold <- 0.6
pca_data$Uncertain <- apply(predictions, 1, max) < uncertainty_threshold

# Step 2: Add the predicted class labels to the PCA data
suppressWarnings({
pca_data$Predicted_Class_nb <- predict(nb_model, newdata = data_enc)
})

# Step 3: Update the ggplot visualization to highlight uncertain points
ggplot(pca_data, aes(x = PC1, y = PC2, color = Predicted_Class_nb, shape = SalePrice_Category)) +
  geom_point(aes(size = !Uncertain), alpha = 0.7) +
  scale_size_manual(values = c(`TRUE` = 1, `FALSE` = 5), guide = "none") +
  labs(title = "Naive Bayes Predictions in 2D (PCA)", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal() +
  scale_color_manual(values = rainbow(length(unique(pca_data$Predicted_Class_nb))))  # Customize colors for predicted classes

```

We see in the plot that many of the "uncertain" points are in the boundary between classes. We also observe that many of these points are all over the middle, since as we have said all throughout the project, it is hard to classify the points in the middle, since a point with High price could be close to one with Low price. Furthermore, many of the points that are uncertain are wrongly classified.

## 3.5 Logistic Regression

### 3.5.1 Introduction

The next algorithm we will use is Logistic Regression. Logistic regression can be used for regression and classification, and there are different types of Logistic Regression. For classification, these are Binary, Multinomial and Ordinal. Binary Logistic Regression has only two possible outcomes, Multinomial has more than two possible outcomes and Ordinal has more than two possible outcomes but with the difference that the possible outcomes have an order.

In this project, we have divided the Price into Very Low, Low, Medium, High, Very High. Clearly, there is an ordering of the classes and there are more than two classes, therefore the best logistic model will be Ordinal. Ordinal Logistic Regression's idea is based on the previous models which we will briefly explain to obtain a general view and then focus on our model.

Logistic regression maps the different futures to probabilities using a sigmoid function to make sure the output is between 0 and 1. It calculates the sigmoid function with a combination of the different variables, which is similar to the regression model. This formula is given by:

$$
\pi(X) = P(Y = 1 \mid X) = \frac{1}{1 + e^{-z}}
$$ Where: $$
z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n
$$

After having all values between 0 and 1, we will use this probability to obtain the odds, which are the relation between the probability of success and failure.

$$
% Sigmoid Function\[\pi(X) = P(Y = 1 \mid X) = \frac{1}{1 + e^{-z}}\]
Where:
\[z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n\]
$$$$\text{Odds} = \frac{\pi(X)}{1 - \pi(X)}$$

But we want the relationship to be linear combination of the variables, based on the previous formula, if we apply the logarithm to get rid of the exponential we will obtain what we want $$\text{Logit}(\pi(X)) = \log\left( \frac{\pi(X)}{1 - \pi(X)} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n$$

Multinomial Logistic Regression uses the Softmax Function which is a generalization of the logistic function in more dimensions. It takes a vector of x numbers and normalizes it into a probability distribution. The output will return components in the interval (0,1) where the components add up to 1. Given by the formula:

$$
\pi_k(X) = \frac{\exp(\beta_{0,k} + \beta_1^T X)}{1 + \sum_{j=1}^{G-1} \exp(\beta_{0,j} + \beta_j^T X)}
$$

The groups, $G$ , in Multinomial Linear Regression go from 0 to $G-1$ where the first group is the reference group $y$ = 0, this could also be the last. The reference class does not have its own betas, it is represented in the denominator. The coefficients for the other groups show how the different variables are related to being in that outcome group versus the reference group. The choosing of this class is important as it affects the coefficients.

Ordinal Logistic Regression has the same groups but they are ordered. We have the function which is a cumulative probability model which measures the probability of being in a category or higher based on the predictors $X$. The main difference is that its probabilities are derived from cumulative probabilities as each category has a threshold whereas Multinomial calculated them independently. The formula is given by

$$
\sum_{g=1}^{G^*} \rho_g = \frac{\exp(\beta_{0,g} + \beta^T X)}{1 + \exp(\beta_{0,g} + \beta^T X)}
$$

### 3.5.2 Training

As explained before we have more than one class, therefore we will not be able to use binary logistic regression. The different groups here show a clear order. But on this project we want to try many different algorithms to understand and interpret them, therefore we will train both Multinomial and Ordinal Logistic regression.

```{r}
# Creating cross validation
train_control <- trainControl(method = "cv", number = 10, verboseIter = FALSE)

# Training the multinomial model with cross-validation
multinomial_model <- train(SalePrice_Category ~ ., data = data_not_enc, method = "multinom", trControl = train_control, trace = FALSE)
# Trace is added to avoid unnecesary output

# Display the results
print(multinomial_model)
```

In this output we see that the multinomial regression is regularized and it uses a decay to obtain better values. Same as seen on QDA, the regularization parameter prevents overfitting and fixes complexity. The accuracy obtained was of 0.69 and a kappa of 0.62 which is a good value and it is slightly better than the models we have seen until now.

We will know use Ordinal Logistic Regression, we will focus more on this model as at first hand it will return the best values

```{r}
ordinal_model1 <- polr(SalePrice_Category ~ ., data = data_not_enc, Hess = TRUE)

## view a summary of the model
summary(ordinal_model1)
```

In this first output we obtain the coefficients with the standard error and t statistic. This value is also done on the different Sale Prices Categories, it compares the log odds of being in a category respect to the reference category. For example, High/Very High is the odds of being in the High category compared to being in the Very High category.

Residual variance is a measure of how well the data is fitted by the model, we want this number to be as low as possible. This number may be useful in the future if we compare the residual variance to other models in order to determine the best model. The AIC is a measure of the quality of the model, the lower the better.

Making the model with cross validation, it already applies regularization

```{r}
# Creating cross validation
train_control <- trainControl(
  method = "cv",
  number = 5
)
# Training the multinomial model with cross-validation
suppressWarnings({
ordinal_model <- train(SalePrice_Category ~ ., data = data_enc, method = "polr", trControl = train_control)
})


# Display the results
print(ordinal_model)
# summary(ordinal_model)
```

We see that there are 5 different link models which use different methods and functions to determine the cumulative properties of the response variables. Ech of these methods behave better depending on the distribution of data, in our case we expect Probit to be the mest method as it assumes a normal distribution. However we see that all methods return really similar values which means they perform really well.

As expected, we obtained a higher value than Multinomial regression, we obtained an accuracy of around 0.72 and a kappa of 0.66 which shows improvement from previous models

### 3.5.3 Interpretation

Following the same procedure as with other algorithms, we will now use PCA to obtain a rep

```{r}
pca_data$Predicted_Class_olr <- predict(ordinal_model, newdata = data_enc)

# Visualize the PCA result with predicted class labels
ggplot(pca_data, aes(x = PC1, y = PC2, color = Predicted_Class_olr, shape=SalePrice_Category)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "RDA Predictions in 2D (PCA)", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal() +
  scale_color_manual(values = rainbow(length(unique(pca_data$Predicted_Class_olr))))  # Customize colors for predicted classes
```

## 3.6 Support Vector Machines

### 3.6.1 Introduction

The next algorithm we will use is SVMs. They are a model that has many applications, but one of the most common is for supervised classification. Its main idea in plain English is to create a hyper-plane that divides each class while maximizing the margin between this hyper-plane and the two classes. Since the classes are not always separable like this, we use the hinge loss function to penalize misclassication. Mathematically:

$$
\mathcal{L}_i = max(0, 1-y_i(\mathbf{w}^T\mathbf{x_i}-b))
$$

Where $\mathbf{w}$ are the weights of the hyper-plane, $x_i$ the features of the observation $i$, $b$ the intercept of the hyper-plane and $y_i$ the target for that observation. With this we see, that when $\mathbf{w}^T\mathbf{x_i}-b$ is very big and has the opposite sign of $y_i$ the loss becomes really big. We then have the following minimization problem:

$$
\min_{\mathbf{w}, b} \, C \sum_{i=1}^n \mathcal{L}_i + \frac{1}{2} \mathbf{w}^T \mathbf{w}
$$

Where $C$ is a hyper-parameter that regulates the trade-off between maximizing the margin and minimizing the hinge loss. This formula maximizes the wideness of the margin, which is given by $\dfrac{2}{\mathbf{w}^T\mathbf{w}}$, while also minimizing the loss function $\mathcal{L}$.

This is a very powerful algorithm, but due to its complexity its use case is for small to medium size datasets which is exactly what we have. Therefore we expect a high improvement in accuracy.

### 3.6.2 Training

We will first use Linear SVM before moving to more complex methods, which assumes that the classes are linearly separable.

```{r}
# Set up train control for cross-validation
train_control <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Train SVM with a linear kernel
svm_linear_model <- train(SalePrice_Category ~ ., 
                          data = data_enc, 
                          method = "svmLinear", 
                          trControl = train_control)  # Scale the features

# Print the results
print(svm_linear_model)
```

As we expected there is a higher accuracy (70%) and kappa (0.63) compared to the previous models. But we know this can be improved, the following way. Before we said that Linear SVM assumes the data is linearly separable but this is not always the case. To solve this, we use the Kernel SVM that augments the dimensionality of the data with a Kernel function in order to do non-linear classification. The kernel function we will use is Gaussian RBF, since is one of the most common and versatile ones. We will also perform a small hyper-parameter tuning to optimize the results further.

```{r}
# Define a grid of hyperparameters to tune
grid <- expand.grid(C = seq(0.7, 2.5, by = 0.1), sigma = seq(0.01, 0.1, by = 0.01))

# Train SVM with grid search
svm_kernel <- train(SalePrice_Category ~ ., 
                    data = data_enc, 
                    method = "svmRadial", 
                    trControl = train_control, 
                    tuneGrid = grid)

# Print the best model
print(svm_kernel)
```

After this implementation of SVMs and the hyper-parameter tuning, we get a 73% accuracy and 0.66 kappa which is the highes yet for any model. This is a slight improvement over the Linear SVM model we trained before too.

### 3.6.3 Interpretation

SVMs are black-box models, meaning that it is difficult to know how the predictions have been done, and therefore interpret our results. Because of this we will use the common plot that we have been doing all over the project to observe how the predictions compare to the real values.

```{r}
# Get predicted class labels using the trained KNN model
pca_data$Predicted_Class_SVM <- predict(svm_kernel, newdata = data_enc)

# Visualize the PCA result with predicted class labels
ggplot(pca_data, aes(x = PC1, y = PC2, color = Predicted_Class_SVM, shape=SalePrice_Category)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "SVM Predictions in 2D (PCA)", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal() +
  scale_color_manual(values = rainbow(length(unique(pca_data$Predicted_Class_SVM))))  # Customize colors for predicted classes
```

Yet again we see the same trend, of difficulty classifying the middle classes, but there is a difference. In this case the class "Medium" has been mostly classified correctly compared with other models. We also observe that overall the classes have been classified correctly for the most part, hence the high accuracy.

## 3.7 Random Forests

### 3.7.1 Introduction

The next algorithm we will see are Random Forests. Random Forest are constructed with many decision trees which were explained previously, they combine bagging (training the model with different bootstrap samples) and random feature selection to obtain a model.

Firstly, it draws $x$ bootstrap samples of size $n$, there is a decision tree for each bootstrap sample. There is a random set of features chosen at each node and the best feature to split on the node is chosen. As our focus is on classification, random forests will select the most popular vote over all the trees to obtain the final prediction.

They are hard to interpret but in exchange they are very accurate and they reduce variance and overfitting.

### 3.7.2 Training

Random forests have parameters such as ntree which correspons to the number of trees created and mtry which corresponds to the number of features selected at each split node. These are some parameters amongs others which could affect the accuracy of the model. Same as decision trees, Random Forests will support categorical data. Firstly, fitting the model.

```{r}
# Fit the Random Forest model
rf_model1 <- randomForest(SalePrice_Category ~ ., 
                      data = data_not_enc, 
                      importance = TRUE)

# Print model summary
print(rf_model1)
```

In the output we see we have OOB estimate of error rate which corresponds to out of bag samples. These are the points that were not selected during bootstrapping. After training the model, each tree can make predictions for the corresponding points, providing an estimate of the performance with unseen data. We can see the value is not really good as we want it to be as low as possible and we have a lot of error.

Moreover, we can see the confusion matrix on the classification. One good thing is that the differences between very high and very low are clearly separated.However there are some low points classified as high and some high classified as low which is not a really bad problem. There is a bigger error in classifying the medium price which is where most points are miss classified.

```{r}
# Set up train control for cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation



# Training Random Forests
rf_model <- train(SalePrice_Category ~ ., 
                          data = data_not_enc, 
                          method = "rf", 
                          ntree = 500,
                          tuneGrid = expand.grid(mtry = c(4,5,6)),
                          maximize = T,
                          trControl = train_control)  

# Print the results
print(rf_model)
```

The best accuracy obtained was of 0.69 with a kappa of 0.61 which is not really good and does not show an improvement with respect to our previous models.

### 3.7.3 Interpretation

Obtaining a feature importance plot, the different variables are classified according to the Gini index from the decision trees.

```{r}
# Extract feature importances
importance_df <- data.frame(
  Feature = row.names(importance(rf_model1)),
  Importance = importance(rf_model1)[, 'MeanDecreaseGini']
)

# Plot Feature Importances
ggplot(importance_df, 
       aes(x = reorder(Feature, Importance), 
           y = Importance)) +
  geom_bar(stat = 'identity', fill = 'steelblue') +
  coord_flip() +
  ggtitle('Feature Importances from Random Forest') +
  xlab('') +
  ylab('Mean Decrease in Gini') +
  theme_minimal()
```

On the one hand, clearly the most relevant features are related with area and quality, neighborhood also seems to be pretty significant. This makes sense, as the larger the area and the better the location the bigger the price will be, and vice versa.

On the other hand, other features such as mszoning and bldg type are not as relevant as the rest of variables but they do provide some useful information. We will now use PCA to plot the classification.

```{r}
pca_data$Predicted_Class_rf <- predict(rf_model, newdata = data_not_enc)

# Visualize the PCA result with predicted class labels
ggplot(pca_data, aes(x = PC1, y = PC2, color = Predicted_Class_rf, shape=SalePrice_Category)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "RDA Predictions in 2D (PCA)", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal() +
  scale_color_manual(values = rainbow(length(unique(pca_data$Predicted_Class_rf))))  # Customize colors for predicted classes
```

```{r}
library(pdp)

# Generate Partial Dependence Plot for 'SalePrice_Category'
pdp_plot <- partial(rf_model, pred.var = "OverallQual", grid.resolution = 20, prob = TRUE, which.class = 3)

# Plot the Partial Dependence Plot
autoplot(pdp_plot, rug = TRUE, train = data_not_enc) +
  ggtitle('Partial Dependence Plot for Overall Quality') +
  xlab('Overall Quality') +
  ylab('Predicted Probability of Medium Price') +
  theme_minimal()

# Generate Partial Dependence Plot for 'SalePrice_Category'
pdp_plot <- partial(rf_model, pred.var = "Neighborhood", grid.resolution = 20, prob = TRUE, which.class = 5)

# Plot the Partial Dependence Plot
autoplot(pdp_plot, rug = TRUE, train = data_not_enc) +
  ggtitle('Partial Dependence Plot for each Neighborhood') +
  xlab('Neighbourhood') +
  ylab('Predicted Probability of Very High Price') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

## 3.8 Gradient Boosting

### 3.8.1 Introduction

Gradient Boosting is an ensemble method used for regression and classification. It builds a strong classifier based on a set of weak learners (decision trees). Although it is computationally expensive and prone to overfitting, models based on this architecture usually are the best performers in many problems, also because modern implementations have mitigated these problems.

Mathematically, what we do is train an ensemble of trees stage-wise and minimizing the loss function iteration over iteration. Mathematically, the typical loss function for multi-class classification (our case) is the following, which is based on the negative log likelihood:

$$
\mathcal{L} = - \sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{1}(y_i = k) \log P(y_i = k \mid x_i)
$$

Where $1(y_i=k)$ is an indicator functions whose value is 1 if $y_i=k$ and 0 otherwise. $P(y_i=k \mid x_i)$ is the probability that the $i$-th observation belongs to group $k$. This probability is calculated using the softmax function:

$$
P(y_i = k \mid x_i) = \dfrac{e^{F_k(x_i)}}{\sum_{j=1}^Ke^{F_j(x_i)}}
$$

Where $F_k(x)$ is the logit produced by the the model for class $k$.

For each class, the model has a function $F_k^{(t)}(x)$, which is updated over iterations, and the starting value could be just log-odds of the class priors. Then we compute the residuals (gradients) as following:

$$
g_{ik}^{(t)} = \frac{\partial \mathcal{L}}{\partial F_k(x_i)} = P(y_i = k \mid x_i) - \mathbb{1}(y_i = k)
$$

With $t$ being the iteration number. After this we fit a decision tree, $h_k^{(t)}$, to predict the negative gradient, so we should end up with something like this: $h_k^{(t)} \approx -g_{ik}^{(t)}$.

We finally update the model with the prediction of this new weak learner, we scale it by a learning rate $\eta$. We do this to make a small-step in the right direction, so that we try to reach a local minimum after not just one operation. So we arrive to the final expression of the model:

$$
F_k^{(t+1)}(x) = F_k^{(t)}(x) + \eta h_k^{(t)} (x)
$$

We end-up doing our prediction $\hat{y}$, as the class with the highest probability in the prediction:

$$
\hat{y} = \arg\max\limits_{x}P(y=k \mid x)
$$

### 3.8.2 Training

Gradient Boosting has many specific algorithms with different implementations. One of the most common ones is XGBoost (eXtreme Gradient Boosting) and is the one we will firstly use. To estimate how our model performs we will use cross-validations. Furthermore, hyper-parameter tuning is crucial in Gradient Boosting so we will also perform it to choose the most optimal model.

```{r}
levels(data_not_enc$SalePrice_Category) <- make.names(levels(data_not_enc$SalePrice_Category))
levels(data_enc$SalePrice_Category) <- make.names(levels(data_enc$SalePrice_Category))

# Define cross-validation method
train_control <- trainControl(
  method = "cv",        # Cross-validation
  number = 5,           # Number of folds
  verboseIter = TRUE,   # Display training progress
  classProbs = TRUE,    # Enable class probabilities
  summaryFunction = multiClassSummary # Evaluate multiple classes
)

# Define the hyperparameter grid
grid <- expand.grid(
  nrounds = c(50, 100, 200),          # Number of boosting iterations
  max_depth = c(3, 5),           # Maximum tree depth
  eta = c(0.01, 0.05, 0.1),            # Learning rate
  gamma = 1,                     # Minimum loss reduction
  colsample_bytree = 0.8,          # Subsample ratio of columns
  min_child_weight = c(1, 3, 5),          # Minimum sum of weights for a child node
  subsample = 0.8      # Subsample ratio of training instances
)

xgb_model <- train(
  SalePrice_Category ~ .,   # Formula: Response ~ Predictors
  data = data_not_enc,      # Training data
  method = "xgbTree",       # XGBoost algorithm
  trControl = train_control, # Cross-validation settings
  tuneGrid = grid,          # Hyperparameter grid
  metric = "Accuracy"       # Optimize for accuracy
)

# Display the best model's parameters
cat("Best Model Hyperparameters:\n")
print(xgb_model$bestTune)

# Display the best accuracy during training (cross-validation)
max(xgb_model$results$Accuracy)
```

We obtain with the best set of hyper-parameters an accuracy of 70% which is surprising since XGBoost is one of the most robust models, and for example the random forest we trained before had a 69% accuracy which is similar.

So, just to check we will use another implementation of Gradient Boosting (GBM), and check the results to see if there is a significant improvement.

```{r}
# Set up trainControl for cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5,  # 5-fold cross-validation
  verboseIter = TRUE,
  classProbs = TRUE,
  summaryFunction = multiClassSummary
)

# Define hyperparameter grid for tuning
gbm_grid <- expand.grid(
  interaction.depth = c(3, 5),  # Depth of the trees
  n.trees = c(50, 100, 200, 500),  # Number of trees
  shrinkage = c(0.01, 0.05, 0.1),  # Learning rate
  n.minobsinnode = c(5, 10)  # Minimum number of observations in a node
)

# Train the gradient boosting model using gbm
gbm_model <- train(
  SalePrice_Category ~ .,
  data = data_not_enc,
  method = "gbm",
  trControl = train_control,
  tuneGrid = gbm_grid,
  metric = "Accuracy"
)

# Display results
print(gbm_model)
cat("Best GBM Accuracy (Cross-Validation):", max(gbm_model$results$Accuracy), "\n")
```

In the end we get a 69% accuracy, which is slightly worse than the first model. With this we observe that Gradient Boosting may not be the best approach for this model, since there are other models that perform slightly better such as SVM or Logistic Regression.

So far, we have used Gradient Boosting for multiple-class classification, but one of the strongest points of this algorithm is regression, which we can use for this problem. Since we wanted to perfor a classification taks, in the preprocessing we divided the price of the houses into different categories. Therefore, if we fit the model for regression and obtain a value, we can then predict the class knowing the limits of the class (e.g. if the model predict a price of 75, and we know that the "High" class are all observation whose price ranges between 50 and 100, the final prediction of this model would be "High").

We will do this implementation and see if there are any significal improvements in the classification.

```{r}
data_regression <- data_enc[, -25]
data_regression$SalePrice <- target_variable

# Step 2: Define the quantiles for classification
quantiles <- quantile(data_regression$SalePrice, probs = seq(0, 1, 0.2), na.rm = TRUE)

# Step 3: Define custom summary function for classification evaluation
classification_summary <- function(data, lev = NULL, model = NULL) {
  # Convert regression predictions to categories
  predicted_category <- cut(
    data$pred,
    breaks = quantiles,
    include.lowest = TRUE,
    labels = c("Very Low", "Low", "Medium", "High", "Very High")
  )
  
  # Calculate confusion matrix metrics
  actual_category <- cut(
    data$obs,
    breaks = quantiles,
    include.lowest = TRUE,
    labels = c("Very Low", "Low", "Medium", "High", "Very High")
  )
  
  cm <- confusionMatrix(predicted_category, actual_category)
  
  # Return relevant classification metrics
  out <- c(Accuracy = cm$overall["Accuracy"])
  return(out)
}

# Step 4: Define trainControl for cross-validation
train_control <- trainControl(
  method = "cv",                      # Cross-validation
  number = 5,                         # 5-fold cross-validation
  verboseIter = TRUE,                 # Print progress
  summaryFunction = classification_summary,  # Use custom summary function
  savePredictions = "final"           # Save predictions
)

xgb_grid <- expand.grid(
  nrounds = c(50, 100, 200),              # Number of boosting rounds
  max_depth = c(3, 5),     # Maximum depth of trees
  eta = c(0.01, 0.05, 0.1),    # Learning rate
  gamma = c(0, 1),                  # Minimum loss reduction
  colsample_bytree = 0.8,       # Subsample ratio of columns
  min_child_weight = c(1, 3, 5),       # Minimum sum of instance weights
  subsample = 0.8             # Subsample ratio of training instances
)

# Step 6: Train the XGBoost regression model with cross-validation
model <- train(
  SalePrice ~ .,                      # Regression formula
  data = data_regression,             # Training data
  method = "xgbTree",                 # Use XGBoost regression
  trControl = train_control,          # Cross-validation control
  tuneGrid = xgb_grid                 # Hyperparameter grid
)

# Step 7: Print the results of the best model
print(model)

# Step 8: View cross-validation results (Accuracy for classification)
results <- model$resample
print(results)

# Step 9: Final classification metrics from cross-validation
mean_accuracy <- mean(results$Accuracy)
cat("Mean Classification Accuracy (via CV):", mean_accuracy, "\n")

```

We observe a very drastical improvement with an almost 74% accuracy, the highest yet. With this we prove that Gradient Boosting is still one of the most robust methods, but we just have to take the right approach.

Although the results are very good, we will not take this model into account. The reason being that, if we were a company trying to earn a profit from predicting the house prices, and we had access to the real value of the houses we would use that instead of separating into classes. This is why we will "forget" that we have the precise valuation of the house and just use the first model we fitted.

### 3.8.3 Interpretation

```{r}
# Extract the feature names from the trained model
feature_names <- xgb_model$finalModel$feature_names

# Compute feature importances using xgb.importance
importance_df <- xgb.importance(
  feature_names = feature_names,
  model = xgb_model$finalModel
)

# Select the top 30 most important features
top_30_features <- importance_df %>%
  arrange(desc(Gain)) %>%  # Sort by Gain in descending order
  head(30)                # Keep only the top 30 features

# Plot the top 30 feature importances
ggplot(top_30_features, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = 'identity', fill = 'steelblue') +
  coord_flip() +
  ggtitle('Top 30 Feature Importances from XGBoost') +
  xlab('Features') +
  ylab('Gain') +
  theme_minimal()

```
