---
title: "Notebook"
output: html_document
date: "2024-11-25"
---

# 1.Set Up

```{r}
rm(list = ls())

# For reproducibility purposes
set.seed(123)

# List of required libraries
required_libraries <- c("skimr", "ggplot2","dplyr", "VIM", "dbscan", "RColorBrewer", "isotree", "Rlof", "e1071", "bestNormalize")

# Install and load libraries
for (lib in required_libraries) {
  if (!require(lib, character.only = TRUE)) {
    install.packages(lib, dependencies = TRUE)
    library(lib, character.only = TRUE)
  }
}
```

We will start obtaining the datasets and interpreting what we have as well as making some decision which could be useful for our future analysis.

```{r}
training <- read.csv("train.csv", sep = ",")
testing <- read.csv("test.csv", sep = ",")
```

```{r}
skim(training)
```

With a first visualization of the dataset we can see that 43 of the columns are categorical while 38 of them are numerical.

# 2. Data Preprocessing

We can see that there are some columns that have many NA values, we will use a barplot to visualize this better and decide whether we should remove columns or not.

## 2.1 NA Values

```{r}
# Plotting a barplot with the number of NA values
barplot(colMeans(is.na(training)), las=2)
```

Based on the previous plot and the skim function used we can see that there are some variables which have NA values for most if not almost all of the observations.

```{r}
# Finidng out the columns with a high number of NA values
colnames(training)[which(colMeans(is.na(training))>0.75)]
```

These columns are Alley, FirePlaceQu, PoolQC, Fence and MiscFeature

Alley refers to the type of alley access to the property. It can be Gravel or Paved, NA means no alley, almost all houses have an NA value here, we don't think that the type of alley is of great importance, so we will remove it.

PoolQC refers to the quality of the swimming pool, as most houses don't have a pool and there are other variables such as PoolArea which refers to the dimension of the swimming pool, we will remove it.

Fence refers to the quality of the fencing, following a similar explanation as before, it has too many NA values, it won't provide much information

MiscFeature mentions other additional features houses may have or not, such as tennis courts. As this are really exclusive and as with the other variables there are a lot of missing values we will delete it.

```{r}
# Deleting the variables
na_columns <- c("Alley","PoolQC","Fence", "MiscFeature")
training <- training %>% select(-all_of(na_columns))

# Obtaining the remaining number of NA values
ncol(training %>% filter(if_any(everything(), is.na)))
```

```{r}
# Obtaining the remaining number of NA values
nrow(training %>% filter(if_any(everything(), is.na)))
```

We can see that 877 observations have NA values, therefore we will now concentrate on replacing the missing values, taking into account their importance and if the missing variable is categorical or not.

To solve missing NA values we will separate the dataset into both numerical and categorical variables, work with both split datasets and then after dealing with the missing values we will merge them.

```{r}
# Separating our dataset into both numerical and categorical data
numeric_data <- select_if(training, is.numeric)
numeric_data <- numeric_data %>% select(-Id)
categorical_data <- select_if(training, is.character)
```

We will start working with numerical data as it will be a little bit easier, firstly we will have a look at the different columns

```{r}
skim(numeric_data)
```

We can see that only 3 variables contain missing values, these are LotFrontage containing 259 missing values, MasVnrArea with 8 and finally GarageYrBlt with 81 values.

About 82% of the observations have a value for LotFrontage which is the variable with more NA values, this is a high completion rate and therefore we will not remove it.

To solve this missing value we will use imputation by the median .

```{r}
# Replace NA values with the median of each column
numeric_data <- numeric_data %>%
  mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))

# Number of rows with NA values 
nrow(numeric_data %>% filter(if_any(everything(), is.na)))
```

Now that the NA values are solved for numeric data, we will have to check for categorical data.

```{r}
skim(categorical_data)
```

The variables with missing values are MasVnrType, all the variables referring to Bsmt which refers to the basement, one NA value in Electrical. Garage variables also have NA values. FirePlaceQu is the variable with most missing values.

As for MasVnrType there are only 8 missing values , Electrical has only one missing value. Due to this we will use KNN to find the most similar points to our missing value and then imputing its most similar value.

Some of the NA values in the basement, garage and FirePlaceQu refer to that the house has none of these elements, therefore we will change the NA values from NA to None. We might use this variables later with encoding to obtain a better analysis and prediction.

```{r}
# Choosing the columns to change from NA to None
na_columns <- c("BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1","BsmtFinType2", "FireplaceQu", "GarageType", "GarageFinish", "GarageQual","GarageCond")


categorical_data[na_columns][is.na(categorical_data[na_columns])] <- "None" 
```

Applying KNN to do the imputation.

```{r}
# Replace NA values with the mode of each column

imputed_categorical_data <- VIM::kNN(categorical_data,
                                variable = c("MasVnrType","Electrical"),
                                k = 10)

# KNN creates a new dataset with extra columns which indicate the rows that have been changed, therefore we will have to change the imputed rows for our original rows

categorical_data$MasVnrType <- imputed_categorical_data$MasVnrType
categorical_data$Electrical <- imputed_categorical_data$Electrical

# Number of rows with NA values 
nrow(categorical_data %>% filter(if_any(everything(), is.na)))

```

```{r}
# Combining both datasets and obtaining a general view of it
data <- cbind(numeric_data, categorical_data)
skim(data)
```

## 2.2 OUTLIERS

We will now deal with potential outliers, to do this we will first visualize the distribution each of the variables follow separating both numerical and categorical data

```{r}
for (col_name in names(numeric_data)) {
  hist(
    numeric_data[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}
  
```

Now we will use different outlier detection methods, we will then combine them to see potential outliers and remove this records.

```{r}


# Step 1: Define the columns with issues
problematic_cols <- c("LowQualFinSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch")

# Step 2: Separate the problematic columns from the numeric data
problematic_data <- numeric_data[, problematic_cols]
new_numeric_data <- numeric_data[, !(names(numeric_data) %in% problematic_cols)]

# Step 3: Scale the remaining numeric data
scaled_num_data <- as.data.frame(apply(new_numeric_data, 2, function(col) {
  bestNormalize(col, allow_orderNorm = TRUE)$x.t
}))
scaled_num_data <- as.data.frame(scale(scaled_num_data)) # Standardize

# Step 4: Scale the problematic columns separately
scaled_problematic <- as.data.frame(scale(problematic_data)) # Standardize without transformation

# Step 5: Combine the scaled dataframes
final_scaled_data <- cbind(scaled_num_data, scaled_problematic)

for (col_name in names(final_scaled_data)) {
  hist(
    final_scaled_data[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}
```

```{r}
# Calculate k-distances (e.g., 4th nearest neighbor)
kNNdistplot(final_scaled_data, k = 4)
abline(h = 6, col = "red", lty = 2)  # Choose the threshold for eps
```

```{r}


model <- dbscan(final_scaled_data, eps = 6, minPts = 4)

pca <- prcomp(final_scaled_data, center = TRUE)

# Get the first two principal components
pca_data <- as.data.frame(pca$x)  # The principal components scores

# Add cluster labels to the PCA data
pca_data$dbscan <- model$cluster

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(dbscan))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "DBSCAN Clustering with Outliers",
       subtitle = "Red points indicate outliers (cluster 0)",
       color = "Cluster") +
  theme_minimal()

sum(pca_data$dbscan == 0)

# Identify Outliers
# outliers <- data[model$cluster == 0, ]
# cleaned_data <- data[model$cluster != 0, ]
```

```{r}
# You may need to install devtools to install this package
#install.packages("devtools")
#devtools::install_github("twitter/AnomalyDetection")

# Load the AnomalyDetection package
#library(AnomalyDetection)

# Example for time series anomaly detection
#result <- AnomalyDetectionVec(scaled_num_data$SalePrice, max_anoms = 0.1, direction = 'both', plot = TRUE)

# View the identified anomalies
#print(result$anoms)

```

Using an Isolation Forest model for outlier detection

```{r}
isolation_model <- isolation.forest(final_scaled_data, ndim = 1, ntrees = 100)
# We could change both ndim and ntrees for different results, but these results are good

outlier_score <- predict(isolation_model, final_scaled_data, type = "score")

# Visualiing the different values
hist(outlier_score, breaks = 20, main = "Distribution of Anomaly Scores")
```

We can see that most of the observations have a value between 0 and 0.5, according to this model a value of 0.5 is the boundary between outliers and the rest of points. Values closer to 1 are potential outliers. We can see that there is one observation far away from the rest while there are some observations between 0.52 and 0.6 which are few but could also be outliers.

```{r}
# Stablishing a threshold for the highest values
threshold <- quantile(outlier_score, 0.95)
outliers <- which(outlier_score >= threshold)
print(unique(outliers)) # DEVUELVE EL MISMO VALOR DOS VECES, POR ESO UNIQUE

# Assuming 'data' is your dataset and 'outliers' contains the indices of outliers
pca_data$iso <- ifelse(1:nrow(pca_data) %in% outliers, 0, 1)
```

Using lof outlier detection model. LOF which stands for Local Outlier Factor. This model measures the local deviation of a point with respect to its neighbor. Points with a much lower density than its neighbor are considered to be outliers. The density of a point is considered to be how crowded or in other words if there are many neighbors around.

The LOF compares the density of a point with its neighbors, a LOF higher than 1 indicates a point in a low density region in relation to its neighbors (potential outlier). A LOF value close to 1 indicates a point which is not an outlier. This is calculated using the reachability distance which is just the maximum of either the distance between two points and the distance of one point and the kth nearest neighbor.

```{r}
# Compute LOF scores
k <- 20  # Number of neighbors
lof_scores <- lof(final_scaled_data, k)

# Identify outliers 
threshold <- quantile(lof_scores, 0.99)
outliers <- which(lof_scores > threshold)

# View results
print(outliers)

```

```{r}
# Visualizing LOF scores
ggplot(data.frame(lof_scores), aes(x = lof_scores)) +
  geom_density(fill = "lightblue", alpha = 0.6) +
  geom_vline(xintercept = threshold, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Density Plot of LOF Scores", x = "LOF Score", y = "Density")

```

```{r}

# Fit the One-Class SVM
ocsvm_model <- svm(
  final_scaled_data,
  type = "one-classification",
  kernel = "radial",  # RBF kernel
  gamma = 1 / ncol(data),  # Default gamma (1 / num_features)
  nu = 0.05  # Proportion of outliers you expect
)

# Get predictions
predictions <- predict(ocsvm_model, final_scaled_data)

# Add the numeric predictions as a column to the PCA data
pca_data$ocsvm <- ifelse(predictions, 1, 0)

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(ocsvm))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "OCSVM Outliers Detection",
       subtitle = "Red points indicate outliers",
       color = "Outlier") +
  theme_minimal()

sum(pca_data$ocsvm == F)


```

```{r}
# Assuming your data has three columns: 'iso', 'ocsvm', and 'dbscan'
# Create the final column
pca_data$outliers <- ifelse(rowSums(pca_data[, c("iso", "ocsvm", "dbscan")]) >= 2, 1, 0)

sum(pca_data$outliers == 0)

# Plot Clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = as.factor(outliers), shape=as.factor(iso))) +
  geom_point(size = 3, alpha = 0.65) +
  scale_color_manual(values = c("red", RColorBrewer::brewer.pal(8, "Set2"))) +
  labs(title = "OCSVM Outliers Detection",
       subtitle = "Red points indicate outliers",
       color = "Outlier") +
  theme_minimal()


```
